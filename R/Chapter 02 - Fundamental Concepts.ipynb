{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fundamental Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.1**. Suppose $\\text{E}[X] = 2$, $\\text{Var}[X] = 9$, $\\text{E}[Y] = 0$, $\\text{Var}[Y] = 4$, and $\\text{Corr}(X, Y) = 0.25$.  Find:\n",
    "\n",
    "**(a)** $\\text{Var}[X + Y]$\n",
    "\n",
    "**(b)** $\\text{Cov}[X, X + Y]$\n",
    "\n",
    "**(c)** $\\text{Corr}[X + Y, X - Y]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** Note that $\\text{Cov}[X, Y] = \\text{Corr}[X, Y] \\sqrt{\\text{Var}[X] \\text{Var}[Y]} = 0.25 \\cdot \\sqrt{9 \\cdot 4} = 1.5$.  Then,\n",
    "\n",
    "$$\\text{Var}[X + Y] = \\text{Var}[X] + \\text{Var}[Y] + \\text{Cov}[X, Y] = 9 + 4 + 1.5 = 14.5$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**\n",
    "\n",
    "$$ \\text{Cov}[X, X + Y] = \\text{Cov}[X, X] + \\text{Cov}[X, Y] = \\text{Var}[X] + \\text{Cov}[X, Y] = 9 + 1.5 = 10.5 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)**\n",
    "\n",
    "Computing the covariance:\n",
    "\n",
    "$$ \\text{Cov}[X + Y, X - Y] = \\text{Cov}[X, X] - \\text{Cov}[X, Y] + \\text{Cov}[X, Y] - \\text{Cov}[Y, Y] = \\text{Var}[X] - \\text{Var}[Y] = 9 - 4 = 5 $$\n",
    "\n",
    "Computing the variance of $X + Y$:\n",
    "\n",
    "$$ \\text{Var}[X + Y] = \\text{Var}[X] + \\text{Var}[Y] + \\text{Cov}[X, Y] = 9 + 4 + 1.5 = 14.5 $$\n",
    "\n",
    "Computing the variance of $X - Y$:\n",
    "\n",
    "$$ \\text{Var}[X - Y] = \\text{Var}[X] + \\text{Var}[Y] - \\text{Cov}[X, Y] = 9 + 4 - 1.5 = 11.5 $$\n",
    "\n",
    "So,\n",
    "\n",
    "$$ \\text{Corr}[X + Y, X - Y] = \\frac{\\text{Cov}[X + Y, X - Y]}{\\sqrt{\\text{Var}[X + Y] \\text{Var}[X - Y]}} = \\frac{5}{\\sqrt{14.5 \\cdot 11.5}} \\approx 0.3872 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.2**.  If $X$ and $Y$ are dependent but $\\text{Var}[X] = \\text{Var}[Y]$, find $\\text{Cov}[X + Y, X - Y]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**.\n",
    "\n",
    "$$ \\text{Cov}[X + Y, X - Y] = \\text{Cov}[X, X] - \\text{Cov}[X, Y] + \\text{Cov}[X, Y] - \\text{Cov}[Y, Y] = \\text{Var}[X] - \\text{Var}[Y] = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.3**.  Let $X$ have a distribution with mean $\\mu$ and variance $\\sigma^2$, and let $Y_t = X$ for all $t$.\n",
    "\n",
    "**(a)** Show that $\\{ Y_t \\}$ is strictly and weakly stationary.\n",
    "\n",
    "**(b)** Find the autocovariance function for $\\{ Y_t \\}$.\n",
    "\n",
    "**(c)** Sketch a \"typical\" time plot of $\\{ Y_t \\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  By definition, the distribution of $Y_{t_1}, Y_{t_2}, \\dots, Y_{t_n}$ is $n$ identical copies of a sample drawn from $X$, which is the same as the distribution of $Y_{t_1 - k}, Y_{t_2 - k}, \\dots, Y_{t_n - k}$ for any $k$, therefore $\\{Y_t\\}$ is strictly stationary.\n",
    "\n",
    "The mean function is $\\mu$ for all $t$, so it is constant over time, and the autocovariance function is $\\gamma_{t, t - k} = \\text{Cov}[Y_t, Y_{t - k}] = \\text{Var}[X] = \\sigma^2$ is also constant for all time $t$ and lag $k$, and so $\\{Y_t\\}$ is weakly stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  As shown in (a), the covariance function is\n",
    "\n",
    "$$ \\gamma_{t, s} = \\text{Cov}[Y_t, Y_s] = \\text{Var}[X] = \\sigma^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)**  A \"typical\" time plot of $\\{ Y_t \\}$ is a constant series, with all values equal to some value drawn from the distribution of $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: ggplot2\n",
      "\n",
      "Loading required package: latex2exp\n",
      "\n"
     ]
    }
   ],
   "source": [
    "require(ggplot2)\n",
    "require(latex2exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABaAAAAHgCAMAAABkaTqFAAAAYFBMVEUAAAAAAP8zMzNNTU1o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",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 240,
       "width": 720
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "options(repr.plot.width=12, repr.plot.height=4)\n",
    "\n",
    "X = rep.int(3, 10)\n",
    "nn = seq(1, 10)\n",
    "\n",
    "ggplot() + \n",
    "  geom_line(aes(x=nn, y=X), color='blue') +\n",
    "  geom_point(aes(x=nn, y=X), color='blue') +\n",
    "  xlab(TeX('t')) + ylab(TeX('Y_t')) +\n",
    "  theme_bw() + theme(text = element_text(size=16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.4**.  Let $\\{ e_t \\}$ be a zero mean white noise process.  Suppose that the observed process is $Y_t = e_t + \\theta e_{t - 1}$, where $\\theta$ is either 3 or 1/3.\n",
    "\n",
    "**(a)** Find the autocorrelation function for $\\{ Y_t \\}$ both when $\\theta = 3$ and when $\\theta = 1/3$.\n",
    "\n",
    "**(b)** You should have discovered that the time series is stationary regardless of the value of $\\theta$ and that the autocorrelation functions are the same for $\\theta = 3$ and $\\theta = 1/3$.  For simplicity, suppose that the process mean is known to be zero and the variance of $Y_t$ is known to be 1.  You observe the series $\\{Y_t\\}$ for $t = 1, 2, \\dots, n$ and suppose that you can produce good estimates of the autocorrelations $\\rho_k$.  Do you think that you could determine which value of $\\theta$ is correct (3 or 1/3) based on the estimate of $\\rho_k$?  Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  The variance is:\n",
    "\n",
    "$$ \\text{Var}[Y_t] = \\text{Var}[e_t + \\theta e_{t - 1}] = \\text{Var}[e_t] + \\theta^2 \\text{Var}[e_{t - 1}] = (1 + \\theta^2) \\sigma_e^2 $$\n",
    "\n",
    "Also\n",
    "\n",
    "$$\n",
    "\\text{Cov}[Y_t, Y_{t - 1}] = \\text{Cov}[e_t + \\theta e_{t - 1}, e_{t - 1} + \\theta e_{t - 2}] = \\theta \\text{Var}[e_{t-1}] = \\theta \\sigma_2^2\n",
    "$$\n",
    "\n",
    "and for element pairs with lag $k > 1$,\n",
    "\n",
    "$$ \\text{Cov}[Y_t, Y_{t - k}] = \\text{Cov}[e_t + \\theta e_{t - 1}, e_{t - k} + \\theta e_{t - k - 1}] = 0 $$\n",
    "\n",
    "Therefore, the autocovariance function is\n",
    "\n",
    "$$ \\gamma_{t, s} = \\begin{cases}\n",
    "(1 + \\theta^2) \\sigma_e^2 &\\text{for } | t - s | = 0 \\\\\n",
    "\\theta \\sigma_e^2         &\\text{for } | t - s | = 1 \\\\\n",
    "0                         &\\text{for } | t - s | > 1\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "and the autocorrelation function is\n",
    "\n",
    "$$ \\rho_{t, s} = \\begin{cases}\n",
    "1                            &\\text{for } | t - s | = 0 \\\\\n",
    "\\theta / (1 + \\theta^2)      &\\text{for } | t - s | = 1 \\\\\n",
    "0                            &\\text{for } | t - s | > 1\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Note that $\\theta / (1 + \\theta^2) = 0.3$ for both $\\theta = 3$ and $\\theta = 1/3$ (this likely being the point of the exercise), so in either scenario the autocorrelation function is\n",
    "\n",
    "$$ \\rho_{t, s} = \\begin{cases}\n",
    "1        &\\text{for } | t - s | = 0 \\\\\n",
    "0.3      &\\text{for } | t - s | = 1 \\\\\n",
    "0        &\\text{for } | t - s | > 1\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  As shown in (a) and stated on the exercise, the series will have the same autocorrelation whether $\\theta = 3$ or $\\theta = 1/3$, so estimates of this property cannot be used to distinguish between the two candidate models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.5**.  Suppose $Y_t = 5 + 2t + X_t$, where $\\{ X_t \\}$ is zero-mean stationary series with autocovariance function $\\gamma_k$.\n",
    "\n",
    "**(a)** Find the mean function for $\\{ Y_t \\}$.\n",
    "\n",
    "**(b)** Find the autocovariance function for $\\{ Y_t \\}$.\n",
    "\n",
    "**(c)** Is $\\{ Y_t \\}$ stationary?  Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** \n",
    "\n",
    "$$ \\mu_t = \\text{E}[Y_t] = \\text{E}[5 + 2t + X_t] = 5 + 2t + \\text{E}[X_t] = 5 + 2t $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** The covariance for terms with lag $k$ is\n",
    "\n",
    "$$ \\text{Cov}[Y_t, Y_{t - k}] = \\text{Cov}[5 + 2t + X_t, 5 + 2(t - k) + X_{t - k}] = \\text{Cov}[X_t, X_{t - k}] = \\gamma_k $$\n",
    "\n",
    "Therefore, the autocovariance function for $\\{Y_t\\}$ is the same as the autocovariance function for $\\{X_t\\}$, $\\gamma_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** The series $\\{Y_t\\}$ is not stationary because the mean is not constant over time -- there is a time drift term, $2t$, in the mean function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.6**.  Let $\\{ X_t \\}$ be a stationary time series, and let \n",
    "\n",
    "$$ Y_t = \\begin{cases}\n",
    "X_t &\\text{for } t \\text{ odd} \\\\\n",
    "X_t + 3 &\\text{for } t \\text{ even}\n",
    "\\end{cases} $$\n",
    "\n",
    "**(a)**  Show that $\\text{Cov}[Y_t, Y_{t - k}]$ is free of $t$ for all lags $k$.\n",
    "\n",
    "**(b)**  Is $\\{ Y_t \\}$ stationary?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  For every possible parity combination of $t$ and $k$ we have:\n",
    "\n",
    "- Even $t$, even $k$:  $\\text{Cov}[Y_t, Y_{t - k}] = \\text{Cov}[X_t + 3, X_{t - k} + 3] = \\text{Cov}[X_t, X_{t - k}] $\n",
    "- Even $t$, odd $k$:  $\\text{Cov}[Y_t, Y_{t - k}] = \\text{Cov}[X_t + 3, X_{t - k}] = \\text{Cov}[X_t, X_{t - k}] $\n",
    "- Odd $t$, even $k$:  $\\text{Cov}[Y_t, Y_{t - k}] = \\text{Cov}[X_t, X_{t - k}] $\n",
    "- Odd $t$, odd $k$:  $\\text{Cov}[Y_t, Y_{t - k}] = \\text{Cov}[X_t, X_{t - k} + 3] = \\text{Cov}[X_t, X_{t - k}] $\n",
    "\n",
    "Since $\\{X_t\\}$ is stationary, $\\text{Cov}[X_t, X_{t - k}]$ is free of $t$, and so $\\text{Cov}[Y_t, Y_{t - k}]$ is free of $t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  No, as its mean function is not constant over time.  Given that $\\{ X_t \\}$ is stationary, it has a constant mean $\\overline{X}$, and we have\n",
    "\n",
    "$$ \\text{E}[Y_t] = \\begin{cases}\n",
    "\\overline{X} &\\text{for } t \\text{ odd} \\\\\n",
    "\\overline{X} + 3 &\\text{for } t \\text{ even}\n",
    "\\end{cases} $$\n",
    "\n",
    "is not constant over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.7**.  Suppose that $\\{Y_t\\}$ is stationary with autocovariance function $\\gamma_k$.\n",
    "\n",
    "**(a)**  Show that $W_t = \\nabla Y_t = Y_t - Y_{t - 1}$ is stationary by finding the mean and the autocovariance function for $\\{ W_t \\}$.\n",
    "\n",
    "**(b)**  Show that $U_t = \\nabla^2 Y_t = \\nabla[Y_t - Y_{t - 1}] = Y_t - 2 Y_{t - 1} + Y_{t - 2}$ is stationary.  (You need not find the mean and autocovariance function for $\\{U_t\\}$.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  Since $\\{Y_t\\}$ has a constant mean $\\overline{Y}$ over time, the mean of $W_t$ is\n",
    "\n",
    "$$ \\text{E}[W_t] = \\text{E}[Y_t - Y_{t - 1}] = \\text{E}[Y_t] - \\text{E}[Y_{t - 1}] = \\overline{Y} - \\overline{Y} = 0 $$\n",
    "\n",
    "The variance of $W_t$ is\n",
    "\n",
    "$$ \\text{Var}[W_t] = \\text{Var}[Y_t - Y_{t - 1}] = \\gamma_1 $$\n",
    "\n",
    "The variance of $W_t$ with lag $k \\geq 1$ is\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\text{Cov}[W_t, W_{t - k}] &= \\text{Cov}[Y_t - Y_{t - 1}, Y_{t - k} - Y_{t - k - 1}] \\\\\n",
    "&= \\text{Cov}[Y_t, Y_{t - k}] - \\text{Cov}[Y_t,  Y_{t - k - 1}] - \\text{Cov}[Y_{t - 1}, Y_{t - k}] + \\text{Cov}[Y_{t - 1}, Y_{t - k - 1}] \\\\\n",
    "&= 2 \\gamma_k - \\gamma_{k + 1} - \\gamma_{k - 1}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "therefore the autocovariance function for $\\{W_t\\}$ is\n",
    "\n",
    "$$ \\omega_k = \\begin{cases}\n",
    "\\gamma_1 &\\text{for } k = 0 \\\\\n",
    "2 \\gamma_k - \\gamma_{k + 1} - \\gamma_{k - 1} &\\text{for } k > 0\n",
    "\\end{cases} $$\n",
    "\n",
    "Since the autocovariance function is free of $k$ and the mean is constant over time, then $\\{W_t\\} is stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  Applying the result of (a) to the series $\\{ W_t \\}$ rather than $ \\{ Y_t \\}$, we get that the series $\\{U_t\\} = \\{ \\nabla W_t \\}$ must also be stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.8**.  Suppose that $\\{Y_t\\}$ is stationary with autocovariance function $\\gamma_k$.  Show that for any positive integer $n$ and any constants $c_1, c_2, \\dots c_n$ the process $\\{ W_t \\}$ defined by $W_t = c_1 Y_t + c_2 Y_{t - 1} + \\cdots + c_n Y_{t - n + 1}$ is stationary.  (Note that Exercise 2.7 is a special case of this result)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**.  If the mean of $\\{Y_t\\}$ is $\\overline{Y}$, then the mean of $\\{W_t\\}$ is\n",
    "\n",
    "$$ \\text{E}[W_t] = \\text{E}\\left[ \\sum_{i=1}^n c_i Y_{t - i + 1} \\right] = \\sum_{i=1}^n c_i \\text{E}[Y_{t - i + 1}] = \\overline{Y} \\sum_{i=1}^n c_i $$\n",
    "\n",
    "which is constant over time.\n",
    "\n",
    "The variance of $W_t$ with lag $k \\geq 0$ is\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\text{Cov}[W_t, W_{t - k}] &= \\text{Cov}\\left[ \\sum_{i=1}^n c_i Y_{t - i + 1}, \\sum_{j=1}^n c_i Y_{t - j + 1 - k}\\right] \\\\\n",
    "&= \\sum_{i=1}^n \\sum_{j=1}^n c_i c_j \\text{Cov} [ Y_{t - i + 1}, Y_{t - j + 1 - k} ]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "and since the difference in the indices of $\\text{Cov} [ Y_{t - i + 1}, Y_{t - j + 1 - k} ]$ does not depend on $t$, no matter which index is largest, then the overall expression is always a linear combination of the autocovariance function $\\gamma_m$ for $0 \\leq m \\leq n + k - 1$,\n",
    "\n",
    "$$ \\text{Cov}[W_t, W_{t - k}] = \\sum_{m=0}^{n + k - 1} d_{m, k} \\gamma_m $$\n",
    "\n",
    "for some constants $d_{m, k}$ that do not depend on $t$.  Therefore, $\\{W_t\\}$ is stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.9**.  Suppose $Y_t = \\beta_0 + \\beta_1 t + X_t$, where $\\{X_t\\}$ is a zero-mean stationary series with autocovariance function $\\gamma_k$ and $\\beta_0$ and $\\beta_1$ are constants.\n",
    "\n",
    "**(a)**  Show that $\\{ Y_t \\}$ is not stationary but that $W_t = \\nabla Y_t = Y_t - Y_{t - 1}$ is stationary.\n",
    "\n",
    "**(b)**  In general, show that if $Y_t = \\mu_t + X_t$, where $\\{ X_t \\}$ is a zero-mean stationary series and $\\mu_t$ is a polynomial in $t$ of degree $d$, then $\\nabla^m Y_t = \\nabla (\\nabla^{m - 1} Y_t)$ is stationary for $m \\geq d$ and nonstationary for $0 \\leq m \\leq d$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  We have\n",
    "\n",
    "$$ \\text{E}[Y_t] = \\text{E}[\\beta_0 + \\beta_1 t + X_t] = \\beta_0 + \\beta_1 t + \\overline{X} $$\n",
    "\n",
    "so the mean of $Y_t$ is not constant in time, and so $\\{Y_t\\}$ is not stationary.\n",
    "\n",
    "On the other hand, the mean of $\\{W_t\\}$ is constant in time,\n",
    "\n",
    "$$ \\text{E}[W_t] = \\text{E}[Y_t - Y_{t - 1}] = \\text{E}[Y_t] - \\text{E}[Y_{t - 1}] = \\beta_1 $$\n",
    "\n",
    "The variance of $W_t$ is\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\text{Var}[W_t] &= \\text{Var}[Y_t - Y_{t - 1}] \\\\\n",
    "&= \\text{Var}[\\beta_0 + \\beta_1 t + X_t - \\beta_0 - \\beta_1 (t - 1) - X_{t - 1}] \\\\\n",
    "&= \\text{Var}[\\beta_1 + X_t - X_{t - 1}] \\\\\n",
    "&= \\text{Var}[X_t - X_{t - 1}] \\\\\n",
    "&= \\text{Var}{X_t} + \\text{Var}[X_{t - 1}] - 2 \\text{Cov}[X_t, X_{t - 1}] \\\\\n",
    "&= 2\\gamma_0 - 2\\gamma_1\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "while the covariance of $W_t$ for lag $k > 0$ is\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\text{Cov}[W_t, W_{t - k}] &= \\text{Cov}[Y_t - Y_{t - 1}, Y_{t - k} - Y_{t - k - 1}] \\\\\n",
    "&= \\text{Cov}[\\beta_0 + \\beta_1 t + X_t - \\beta_0 - \\beta_1 (t - 1) - X_{t - 1}, \n",
    "\\beta_0 + \\beta_1 (t - k) + X_{t - k} - \\beta_0 - \\beta_1 (t - k - 1) - X_{t - k - 1}] \\\\\n",
    "&= \\text{Cov}[\\beta_1 + X_t - X_{t - 1}, \\beta_1 + X_{t - k} -  X_{t - k - 1}] \\\\\n",
    "&= \\text{Cov}[X_t - X_{t - 1}, X_{t - k} -  X_{t - k - 1}] \\\\\n",
    "&= \\text{Cov}[X_t, X_{t - k}] - \\text{Cov}[X_t, X_{t - k - 1}] - \\text{Cov}[X_{t - 1}, X_{t - k - 1}] + \\text{Cov}[X_{t - 1}, X_{t - k - 1}] \\\\\n",
    "&= 2\\gamma_k - \\gamma_{k + 1} - \\gamma_{k - 1}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "and so the autocovariance function for $\\{W_t\\}$ does not depend on $t$,\n",
    "\n",
    "$$ \\omega_k = \\begin{cases}\n",
    "\\gamma_1 &\\text{for } k = 0 \\\\\n",
    "2 \\gamma_k - \\gamma_{k + 1} - \\gamma_{k - 1} &\\text{for } k > 0\n",
    "\\end{cases} $$\n",
    "\n",
    "Since $\\{W_t\\}$ has a constant mean in time and an autocovariance function that does not depend on time, it is stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  \n",
    "\n",
    "**Lemma 2.9.1**: if $\\{Y_t\\}$ is stationary, then $\\{ \\nabla Y_t \\}$ must also be stationary.\n",
    "\n",
    "**Proof**:\n",
    "\n",
    "- The mean of $\\{ \\nabla Y_t \\}$ constant (and zero):\n",
    "\n",
    "$$ \\text{E}[\\nabla Y_t] = \\text{E}[Y_t - Y_{t - 1}] = \\text{E}[Y_t] - \\text{E}[Y_{t - 1}] = \\overline{Y} - \\overline{Y} = 0 $$\n",
    "\n",
    "- Assuming $\\{Y_t\\}$ has autocovariance function $\\gamma_k$, the variance of $\\{ \\nabla Y_t \\}$ is:\n",
    "\n",
    "  $$ \\text{Var}[ \\nabla Y_t ] = \\text{Var}[Y_t - Y_{t - 1}] = \\text{Var}{Y_t} + \\text{Var}[Y_{t - 1}] - 2 \\text{Cov}[Y_t, Y_{t - 1}] = 2\\gamma_0 - 2 \\gamma_1 $$\n",
    "\n",
    "  and the autocovariance for lag $k > 0$ is\n",
    "  \n",
    "  $$ \n",
    "  \\begin{align}\n",
    "  \\text{Cov}[\\nabla Y_t, \\nabla Y_{t - k}] &= \\text{Cov}[Y_t - Y_{t - 1}, Y_{t - k} - Y_{t - k - 1}] \\\\\n",
    "  &= \\text{Cov}[Y_t, Y_{t - k}] - \\text{Cov}[Y_t, Y_{t - k - 1}] - \\text{Cov}[Y_{t - 1}, Y_{t - k - 1}] + \\text{Cov}[Y_{t - 1}, Y_{t - k - 1}] \\\\\n",
    "&= 2\\gamma_k - \\gamma_{k + 1} - \\gamma_{k - 1}\n",
    "  \\end{align}\n",
    "  $$\n",
    "  \n",
    "Therefore $\\{\\nabla Y_t\\}$ has a constant mean and an autocovariance function that does not depend on time, and so it is stationary.\n",
    "\n",
    "**Lemma 2.9.2**: If $\\mu_t$ is a polynomial of degree $d$, then $\\nabla^k \\mu_t = \\nabla(\\nabla^{k - 1}\\mu_t - \\nabla^{k - 1}\\mu_{t - 1})$ is a polynomial of degree $d - 1$, for $1 \\leq k \\leq d$.\n",
    "\n",
    "**Proof**: Let $\\mu_t = \\sum_{j=0}^d c_j t^j$, for constants $c_j$.  \n",
    "\n",
    "- For $k = 1$, $\\nabla \\mu_t = \\mu_t - \\mu_{t - 1} = \\sum_{j=1}^d c_j (t^j - (t - 1)^j) $.  In each term, the coefficients of $t^j$ in $t^j$ and $(t - 1)^j$ cancel out, so $t^j - (t - 1)^j$ is a polynomial of degree $j - 1$, and the overall expression is a polynomial of degree $d - 1$.\n",
    "- For $k > 1$, $\\nabla^k \\mu_t = \\nabla(\\nabla^{k - 1}\\mu_t - \\nabla^{k - 1}\\mu_{t - 1}) = \\nabla (a_t - a_{t - 1})$, where $a_t$ is a polynomial of degree $d - k + 1$ by induction, therefore $\\nabla^k \\mu_t$ is a polynomial of degree $d - k$.\n",
    "\n",
    "Now, we can prove the result from (b) by induction.  \n",
    "\n",
    "- Base case $d = 0$:  This is equivalent to Lemma 2.9.1.\n",
    "\n",
    "- Base case $d = 1$:  This is equivalent to the result in (a).\n",
    "\n",
    "- Induction step:\n",
    "  - If $m < d$, then $\\nabla_m Y_t$ is a polynomial $A$ in $t$ with degree $d - m$, plus a linear combination of $X_t, X_{t - 1}, \\dots, X_{t - m}$.  Therefore the expected value of $\\nabla_m Y_t$ is  $\\text{E}[A(t) - A(t - 1) + \\sum c_j X_j] = A(t) - A(t - 1) + \\overline{X} \\sum c_j$, which is a non-constant function of $t$.  Therefore the mean is not constant in time, and so $\\{ \\nabla^m Y_t \\}$ is not stationary.\n",
    "  - If $m > d$, then $\\nabla_m Y_t$ is a linear combination of $X_t, X_{t - 1}, \\dots, X_{t - m}$ (using Lemma 2.9.2, applying $\\nabla$ enough times zeroes out the polynomial component).  Therefore, from Lemma 2.9.1, $\\{ \\nabla^m Y_t \\}$ is stationary.\n",
    "  - If $m = d$, then $\\nabla_m Y_t$ is a zero degree polynomial (a constant) plus a linear combination of of $X_t, X_{t - 1}, \\dots, X_{t - m}$ (using Lemma 2.9.2, applying $\\nabla$ enough times to bring the degree down to exactly 0).  Therefore, the expectation of $\\nabla^m Y_t$ is that constant plus $\\overline{X} \\sum c_j$, and so it does not change with time.  The zero-polynomial constant only scales up the covariance terms by a constant factor, and so the autocovariance also does not depend on the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.10**.  Let $\\{X_t\\}$ be a zero-mean, unit-variance stationary process with autocorrelation function $\\rho_k$.  Suppose that $\\mu_t$ is a nonconstant function and that $\\sigma_t$ is a positive-valued nonconstant function.  The observed series is formed as $Y_t = \\mu_t + \\sigma_t X_t$.\n",
    "\n",
    "**(a)**  Find the mean and the covariance function for the $\\{ Y_t \\}$ process.\n",
    "\n",
    "**(b)**  Show that the autocorrelation function for the $\\{ Y_t \\}$ process depends only on the time lag.  Is the $\\{Y_t\\}$ process stationary?\n",
    "\n",
    "**(c)**  Is it possible to have a time series with a constant mean and with $\\text{Corr}[Y_t, Y_{t - k}]$ free of $t$ but with $\\{Y_t\\}$ not stationary?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  The mean of the process is:\n",
    "\n",
    "$$ \\text{E}[Y_t] = \\text{E}[\\mu_t + \\sigma_t X_t] = \\mu_t + \\sigma_t \\text{E}[X_t] = \\mu_t $$\n",
    "\n",
    "and the covariance function is\n",
    "\n",
    "$$ \\text{Cov}[Y_t, Y_s] = \\text{Cov}[\\mu_t + \\sigma_t X_t, \\mu_s + \\sigma_s X_s] = \\sigma_t \\sigma_s \\text{Cov}[X_t, X_s] = \\sigma_t \\sigma_s \\rho_{t - s}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  The autocorrelation for the $\\{ Y_t \\}$ process is\n",
    "\n",
    "$$ \\text{Corr}[Y_t, Y_{t - k}] = \\frac{\\text{Cov}[Y_t, Y_{t - k}]}{\\sqrt{\\text{Var}[Y_t] \\text{Var}[Y_{t - k}]}} = \\frac{\\sigma_t \\sigma_{t - k} \\rho_k}{\\sqrt{\\sigma_t^2 \\sigma_{t - k}^2}} = \\rho_k $$\n",
    "\n",
    "that is, it is the same as the autocorrelation for the $\\{X_t\\}$ process, and it depends only on the time lag.\n",
    "\n",
    "Note that this does not make the process stationary, both because it has nonconstant mean $\\mu_t$ and because stationarity requires time-independent autocovariance, not time-independent autocorrelation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** Yes -- for example, take $\\{X_t\\}$ as some series with more than two distinct non-zero autocorrelation values, make $\\mu_t = 0$ and $\\sigma_t = t$.  Then, even though $\\{Y_t\\}$ has constant mean 0, the autocovariance depends on the time,\n",
    "\n",
    "$$ \\text{Cov}[Y_t, Y_{t - k}] = \\sigma_t \\sigma_{t - k} \\rho_k = t(t-k) \\rho_k$$\n",
    "\n",
    "despite that $\\text{Corr}[Y_t, Y_{t - k}] = \\rho_k$ is free of $t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.11**.  Suppose $\\text{Cov}[X_t, X_{t - k}] = \\gamma_k$ is free of $t$ but that $\\text{E}[X_t] = 3t$.\n",
    "\n",
    "**(a)** Is $\\{ X_t \\}$ stationary?\n",
    "\n",
    "**(b)** Let $Y_t = 7 - 3t + X_t$.  Is $\\{Y_t\\}$ stationary?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  No.  By definition, a series with a time-dependant mean is not stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  Yes.  We have that $\\{ Y_t \\}$ has constant mean,\n",
    "\n",
    "$$ \\text{E}[Y_t] = \\text{E}[7 - 3t + X_t] = 7 - 3t + \\text{E}[X_t] = 7 $$\n",
    "\n",
    "and its autocovariance function is\n",
    "\n",
    "$$ \\text{Cov}[Y_t, Y_{t - k}] = \\text{Cov}[7 - 3t + X_t, 7 - 3(t - k) + X_{t - k}] = \\text{Cov}[X_t, X_{t - k}] = \\gamma_k $$\n",
    "\n",
    "which is free of $t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.12**.  Suppose that $Y_t = e_t - e_{t - 12}$.  Show that $\\{Y_t\\}$ is stationary and that, for $k > 0$, its autocorrelation function is nonzero only for lag $k = 12$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**.  We have:\n",
    "\n",
    "$$ \\text{E}[Y_t] = \\text{E}[e_t - e_{t - 12}] = \\text{E}[e_t] - \\text{E}[e_{t - 12}] = \\overline{e} - \\overline{e} = 0 $$\n",
    "\n",
    "so the series has constant mean.\n",
    "\n",
    "The variance is\n",
    "\n",
    "$$ \\text{Var}[Y_t] = \\text{Var}[e_t - e_{t - 12}] = \\text{Var}[e_t] + \\text{Var}[e_{t - 12}] = 2 \\sigma_e^2 $$\n",
    "\n",
    "and for $k > 0$, its autocovariance function is \n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\text{Cov}[Y_t, Y_{t - k}] &= \\text{Cov}[e_t - e_{t - 12}, e_{t - k} - e_{t - k - 12}] \\\\\n",
    "&= \\text{Cov}[e_t, e_{t - k}] - \\text{Cov}[e_{t - 12}, e_{t - k}] - \\text{Cov}[e_t, e{t - k - 12}] + \\text{Cov}[e_t, e_{t - k - 12}] \\\\\n",
    "&= -\\text{Cov}[e_{t - 12}, e_{t - k}]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "which is zero if $k \\neq 12$ and $-\\sigma_e^2$ otherwise.\n",
    "\n",
    "Therefore, the autocovariance function is\n",
    "\n",
    "$$ \n",
    "\\gamma_k = \\begin{cases}\n",
    "2 \\sigma_e^2 &\\text{for } k = 0 \\\\\n",
    "-\\sigma_e^2 &\\text{for } k = 12 \\\\\n",
    "0 &\\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "and the autocorrelation function is\n",
    "\n",
    "$$ \n",
    "\\rho_k = \\begin{cases}\n",
    "1 &\\text{for } k = 0 \\\\\n",
    "-1/2 &\\text{for } k = 12 \\\\\n",
    "0 &\\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Since the mean is constant and the autocovariance does not depend on the time, the series is stationary.  We have also shown that the only lag with a non-zero autocorrelation value is for $k = 12$, when the autocorrelation term is -1/2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.13**.  Let $Y_t = e_t - \\theta (e_{t - 1})^2$.  For this exercise, assume that the white noise series is normally distributed.\n",
    "\n",
    "**(a)** Find the autocorrelation function for $Y_t$.\n",
    "\n",
    "**(b)** Is $\\{Y_t\\}$ stationary?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  The mean of $\\{ Y_t \\}$ is\n",
    "\n",
    "$$ \\text{E}[Y_t] = \\text{E}[e_t - \\theta (e_{t - 1})^2] = \\text{E}[e_t] - \\theta \\text{E}[e_{t-1}^2] = -\\theta \\sigma_e^2 $$\n",
    "\n",
    "The variance of $\\{ Y_t \\}$ is\n",
    "\n",
    "$$ \\text{Var}[Y_t] = \\text{Var}[e_t - \\theta (e_{t - 1})^2] = \\text{Var}[e_t] + \\theta^2 \\text{Var}[e_{t-1}^2] = \\sigma_e^2 + \\theta^2 \\text{E}[e_{t-1}^4] = \\sigma_e^2 + 3 \\theta^2 \\sigma_e^4 $$\n",
    "\n",
    "where we used the fact that the fourth moment of a normal distribution $N(0, \\sigma^2)$ is $\\text{E}[X^4] = 3 \\sigma^4$.\n",
    "\n",
    "The autocovariance of $\\{ Y_t \\}$ for lag $k > 1$ is\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\text{Cov}[Y_t, Y_{t - k}] &= \\text{Cov}[e_t - \\theta (e_{t - 1})^2, e_{t - k} - \\theta (e_{t - k - 1})^2] \\\\\n",
    "&= \\text{Cov}[e_t, e_{t - k}] - \\theta \\text{Cov}[e_t, (e_{t - k - 1})^2] - \\theta \\text{Cov}[(e_{t - 1})^2, e_{t - k}] + \\theta^2 \\text{Cov}[(e_{t - 1})^2, (e_{t - k - 1})^2] \\\\\n",
    "&= 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "since all random variables within the covariances are independent.\n",
    "\n",
    "The autocovariance of $\\{ Y_t \\}$ for lag $k = 1$ is\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\text{Cov}[Y_t, Y_{t - 1}] &= \\text{Cov}[e_t - \\theta (e_{t - 1})^2, e_{t - 1} - \\theta (e_{t - 2})^2] \\\\\n",
    "&= \\text{Cov}[e_t, e_{t - 1}] - \\theta \\text{Cov}[e_t, (e_{t -2})^2] - \\theta \\text{Cov}[(e_{t - 1})^2, e_{t - 1}] + \\theta^2 \\text{Cov}[(e_{t - 1})^2, (e_{t - 2})^2] \\\\\n",
    "&= - \\theta \\text{Cov}[(e_{t - 1})^2, e_{t - 1}] \\\\\n",
    "&= -\\theta \\left( \\text{E}[e_{t - 1}^3] - \\text{E}[e_{t - 1}^2] \\text{E}[e_{t - 1}]\\right) \\\\\n",
    "&= 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where we used the fact that the third moment of a normal distribution  $N(0, \\sigma^2)$ is $\\text{E}[X^3] = 0$ and the first moment of the distribution is its mean, which is also zero.\n",
    "\n",
    "Therefore, the autocovariance of $\\{Y_t\\}$ is\n",
    "\n",
    "$$\n",
    "\\gamma_k = \\begin{cases}\n",
    "\\sigma_e^2 + 3 \\theta^2 \\sigma_e^4 &\\text{for } k = 0 \\\\\n",
    "0 &\\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "and the autocorrelation is\n",
    "\n",
    "$$\n",
    "\\rho_k = \\begin{cases}\n",
    "1 &\\text{for } k = 0 \\\\\n",
    "0 &\\text{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  Yes.  The series has a constant mean $\\overline{Y} = \\theta \\sigma_e^2$, and an autocovariance which does not depend on $t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.14**.  Evaluate the mean and the covariance function for each of the following processes.  In each case, determine whether or not the process is stationary.\n",
    "\n",
    "**(a)**  $Y_t = \\theta_0 + t e_t$.\n",
    "\n",
    "**(b)**  $W_t = \\nabla Y_t$, where $Y_t$ is given in part (a).\n",
    "\n",
    "**(c)**  $Y_t = e_t e_{t - 1}$.  (You may assume that $\\{e_t\\}$ is normal white noise.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  The mean of the process is\n",
    "\n",
    "$$ \\text{E}[Y_t] = \\text{E}[\\theta_0 + t e_t] = \\theta_0 + t \\text{E}[e_t] = \\theta_0 $$\n",
    "\n",
    "The variance of the process is\n",
    "\n",
    "$$ \\text{Var}[Y_t] = \\text{Var}[\\theta_0 + t e_t] = t^2 \\text{Var}[e_t] = t^2 \\sigma_e^2 $$\n",
    "\n",
    "which is a function of time, and so the process cannot be stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  The mean of the process is\n",
    "\n",
    "$$ \\text{E}[\\nabla Y_t] = \\text{E}[Y_t - Y_{t - 1}] = \\text{E}[Y_t] - \\text{E}[Y_{t - 1}] = \\theta_0 - \\theta_0 = 0 $$\n",
    "\n",
    "The variance of the process is\n",
    "\n",
    "$$ \\text{Var}[\\nabla Y_t] = \\text{Var}[Y_t - Y_{t - 1}] = \\text{Var}[Y_t] + \\text{Var}[Y_{t - 1}] = t^2 \\sigma_e^2 + (t - 1)^2 \\sigma_e^2 = (2t^2 + 2t - 1) \\sigma_e^2 $$\n",
    "\n",
    "which is a function of time, and so the process cannot be stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)**  The mean of the process is\n",
    "\n",
    "$$ \\text{E}[Y_t] = \\text{E}[e_t e_{t-1}] = \\text{E}[e_t] \\text{E}[e_{t - 1}] = 0 $$\n",
    "\n",
    "The variance of the process is\n",
    "\n",
    "$$ \\text{Var}[Y_t] = \\text{Var}[e_t e_{t - 1}] = \\text{E}[e_t^2 e_{t - 1}^2] = \\text{E}[e_t^2] \\text{E}[ e_{t - 1}^2] = \\sigma_e^2 $$\n",
    "\n",
    "The autocorrelation of the process with lag $k > 0$ is\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\text{Cov}[Y_t, Y_{t - k}] &= \\text{E}[Y_t Y_{t - k}] - \\text{E}[Y_t] \\text{E}[Y_{t - k}]  \\\\\n",
    "&= \\text{E}[e_t e_{t - 1} e_{t - k} e_{t - k - 1}] - 0 \\cdot 0 \\\\\n",
    "&= \\text{E}[e_t] \\text{E}[e_{t - 1} e_{t - k} e_{t - k - 1}] \\\\\n",
    "&= 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Therefore, the autocovariance of the process is\n",
    "\n",
    "$$ \n",
    "\\gamma_k = \\begin{cases}\n",
    "\\sigma_e^2 &\\text{for } k = 0 \\\\\n",
    "0 &\\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Since the process has a constant mean and autocovariance that is free of time $t$, the process is stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.15**.  Suppose that $X$ is a random variable with zero mean.  Define a time series by $Y_t = (-1)^t X$.\n",
    "\n",
    "**(a)** Find the mean function for $\\{ Y_t \\}$.\n",
    "\n",
    "**(b)** Find the covariance function for $\\{ Y_t \\}$.\n",
    "\n",
    "**(c)** Is $\\{Y_t\\}$ stationary?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  The mean function is\n",
    "\n",
    "$$ \\text{E}[Y_t] = \\text{E}[(-1)^t X] = (-1)^t \\text{E}[X] = (-1)^t \\cdot 0 = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  The variance is\n",
    "\n",
    "$$ \\text{Var}[Y_t] = \\text{E}[Y_t^2] - \\text{E}[Y_t]^2 = \\text{E}[(-1)^{2t}X^2]  - 0^2= \\text{E}[X^2] = \\text{Var}[X] = \\sigma_X^2 $$\n",
    "\n",
    "The covariance for even lag $k$ is\n",
    "\n",
    "$$ \\text{Cov}[Y_t, Y_{t - k}] = \\text{Cov}[Y_t, Y_t] = \\text{Var}[Y_t] = \\sigma_X^2 $$\n",
    "\n",
    "and the covariance for odd lag $k$ is\n",
    "\n",
    "$$ \\text{Cov}[Y_t, Y_{t - k}] = \\text{Cov}[Y_t, -Y_t] = -\\text{Var}[Y_t] = -\\sigma_X^2 $$\n",
    "\n",
    "Therefore the covariance function is\n",
    "\n",
    "$$ \n",
    "\\gamma_{t, s} = \\begin{cases}\n",
    "\\sigma_X^2 &\\text{for } |t - s| \\text{ even} \\\\\n",
    "-\\sigma_X^2 &\\text{for } |t - s| \\text{ odd}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)**  Yes.  The mean is constant, and the autocovariance function is $t$ free, depending only on the lag:\n",
    "\n",
    "$$ \\gamma_k = \\begin{cases}\n",
    "\\sigma_X^2 &\\text{for } k \\text{ even} \\\\\n",
    "-\\sigma_X^2 &\\text{for } k \\text{ odd} \\\\\n",
    "\\end{cases} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.16**.  Suppose $Y_t = A + X_t$, where $\\{X_t\\}$ is stationary and $A$ is random but independent of $\\{ X_t \\}$.  Find the mean and covariance function for $\\{Y_t\\}$ in terms of the mean and autocovariance function for $\\{ X_t \\}$ and the mean and variance of $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**.\n",
    "\n",
    "The mean is\n",
    "\n",
    "$$ \\text{E}[Y_t] = \\text{E}[A + X_t] = \\text{E}[A] + \\text{E}[X_t] = \\mu_A + \\mu_X $$\n",
    "\n",
    "where $\\mu_A, \\mu_X$ are the means of $A$ and $\\{X_t\\}$.\n",
    "\n",
    "The variance is\n",
    "\n",
    "$$ \\text{Var}[Y_t] = \\text{Var}[A + X_t] = \\text{Var}[A] + \\text{Var}[X_t] = \\sigma_A^2 + \\sigma_X^2 $$\n",
    "\n",
    "The covariance is\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\text{Cov}[Y_t, Y_s] &= \\text{Cov}[A + X_t, A + X_s]\n",
    "&= \\text{Cov}[A, A] + \\text{Cov}[A, X_s] + \\text{Cov}[X_t, A] + \\text{Cov}[X_t, X_s] \\\\\n",
    "&= \\sigma_A^2 + \\gamma_{t, s}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Therefore, the autocovariance function for $\\{Y_t\\}$ is\n",
    "\n",
    "$$ \\omega_k = \\sigma_A^2 + \\gamma_k $$\n",
    "\n",
    "where $\\gamma_k$ is the autocovariance function for $\\{X_t\\}$ and $\\sigma_A^2$ is the variance of $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.17**.  Let $\\{ Y_t \\}$ be stationary with autocovariance function $\\gamma_k$.  Let $\\overline{Y} = \\frac{1}{n} \\sum_{t=1}^n Y_t$.  Show that\n",
    "\n",
    "$$ \\text{Var}[\\overline{Y}] = \\frac{\\gamma_0}{n} + \\frac{2}{n} \\sum_{k=1}^{n -1} \\left( 1 - \\frac{k}{n} \\right) \\gamma_k \n",
    "= \\frac{1}{n} \\sum_{k=-n+1}^{n - 1} \\left(1 - \\frac{|k|}{n} \\right) \\gamma_k\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**.  We have\n",
    "\n",
    "$$ \n",
    "\\text{Var}[\\overline{Y}] = \\text{Var}\\left[ \\frac{1}{n} \\sum_{t=1}^n Y_i \\right] = \\frac{1}{n^2} \\sum_{i=1}^n \\sum_{j=1}^n \\text{Cov}[Y_i, Y_j] = \\frac{1}{n^2} \\sum_{i=1}^n \\sum_{j=1}^n \\gamma_{|j - i|}\n",
    "$$\n",
    "\n",
    "The term with lag $k = |j - i|$ appears $2(n - k)$ times, as that is the number of ways of selecting $i, j$ in the summation intervals to produce that lag.  The equalities in the result follow as a consequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.18**.  Let $\\{ Y_t \\}$ be stationary with autocovariance function $\\gamma_k$.  Define the sample variance as $ S^2 = \\frac{1}{n - 1} \\sum_{t=1}^n (Y_t - \\overline{Y})^2$.\n",
    "\n",
    "**(a)** First show that $\\sum_{t=1}^n (Y_t - \\mu)^2 = \\sum_{t = 1}^n (Y_t - \\overline{Y})^2 + n(\\overline{Y} - \\mu)^2$.\n",
    "\n",
    "**(b)** Use part (a) to show that\n",
    "\n",
    "**(c)**  $$ \\text{E}[S^2] = \\frac{n}{n - 1} \\gamma_0 - \\frac{n}{n - 1} \\text{Var}[\\overline{Y}] = \\gamma_0 - \\frac{2}{n - 1} \\sum_{k=1}^{n - 1} \\left( 1 - \\frac{n}{k} \\right) \\gamma_k $$\n",
    "\n",
    "(Use the results of Exercise 2.17 for the last expression.)\n",
    "\n",
    "**(d)** It $\\{Y_t \\}$ is a white noise process with variance $\\gamma_0$, show that $\\text{E}[S^2] = \\gamma_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
