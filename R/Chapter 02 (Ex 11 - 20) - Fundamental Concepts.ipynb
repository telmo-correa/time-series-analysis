{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.11**.  Suppose $\\text{Cov}[X_t, X_{t - k}] = \\gamma_k$ is free of $t$ but that $\\text{E}[X_t] = 3t$.\n",
    "\n",
    "**(a)** Is $\\{ X_t \\}$ stationary?\n",
    "\n",
    "**(b)** Let $Y_t = 7 - 3t + X_t$.  Is $\\{Y_t\\}$ stationary?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  No.  By definition, a series with a time-dependant mean is not stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  Yes.  We have that $\\{ Y_t \\}$ has constant mean,\n",
    "\n",
    "$$ \\text{E}[Y_t] = \\text{E}[7 - 3t + X_t] = 7 - 3t + \\text{E}[X_t] = 7 $$\n",
    "\n",
    "and its autocovariance function is\n",
    "\n",
    "$$ \\text{Cov}[Y_t, Y_{t - k}] = \\text{Cov}[7 - 3t + X_t, 7 - 3(t - k) + X_{t - k}] = \\text{Cov}[X_t, X_{t - k}] = \\gamma_k $$\n",
    "\n",
    "which is free of $t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.12**.  Suppose that $Y_t = e_t - e_{t - 12}$.  Show that $\\{Y_t\\}$ is stationary and that, for $k > 0$, its autocorrelation function is nonzero only for lag $k = 12$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**.  We have:\n",
    "\n",
    "$$ \\text{E}[Y_t] = \\text{E}[e_t - e_{t - 12}] = \\text{E}[e_t] - \\text{E}[e_{t - 12}] = \\overline{e} - \\overline{e} = 0 $$\n",
    "\n",
    "so the series has constant mean.\n",
    "\n",
    "The variance is\n",
    "\n",
    "$$ \\text{Var}[Y_t] = \\text{Var}[e_t - e_{t - 12}] = \\text{Var}[e_t] + \\text{Var}[e_{t - 12}] = 2 \\sigma_e^2 $$\n",
    "\n",
    "and for $k > 0$, its autocovariance function is \n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\text{Cov}[Y_t, Y_{t - k}] &= \\text{Cov}[e_t - e_{t - 12}, e_{t - k} - e_{t - k - 12}] \\\\\n",
    "&= \\text{Cov}[e_t, e_{t - k}] - \\text{Cov}[e_{t - 12}, e_{t - k}] - \\text{Cov}[e_t, e{t - k - 12}] + \\text{Cov}[e_t, e_{t - k - 12}] \\\\\n",
    "&= -\\text{Cov}[e_{t - 12}, e_{t - k}]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "which is zero if $k \\neq 12$ and $-\\sigma_e^2$ otherwise.\n",
    "\n",
    "Therefore, the autocovariance function is\n",
    "\n",
    "$$ \n",
    "\\gamma_k = \\begin{cases}\n",
    "2 \\sigma_e^2 &\\text{for } k = 0 \\\\\n",
    "-\\sigma_e^2 &\\text{for } k = 12 \\\\\n",
    "0 &\\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "and the autocorrelation function is\n",
    "\n",
    "$$ \n",
    "\\rho_k = \\begin{cases}\n",
    "1 &\\text{for } k = 0 \\\\\n",
    "-1/2 &\\text{for } k = 12 \\\\\n",
    "0 &\\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Since the mean is constant and the autocovariance does not depend on the time, the series is stationary.  We have also shown that the only lag with a non-zero autocorrelation value is for $k = 12$, when the autocorrelation term is -1/2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.13**.  Let $Y_t = e_t - \\theta (e_{t - 1})^2$.  For this exercise, assume that the white noise series is normally distributed.\n",
    "\n",
    "**(a)** Find the autocorrelation function for $Y_t$.\n",
    "\n",
    "**(b)** Is $\\{Y_t\\}$ stationary?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  The mean of $\\{ Y_t \\}$ is\n",
    "\n",
    "$$ \\text{E}[Y_t] = \\text{E}[e_t - \\theta (e_{t - 1})^2] = \\text{E}[e_t] - \\theta \\text{E}[e_{t-1}^2] = -\\theta \\sigma_e^2 $$\n",
    "\n",
    "The variance of $\\{ Y_t \\}$ is\n",
    "\n",
    "$$ \\text{Var}[Y_t] = \\text{Var}[e_t - \\theta (e_{t - 1})^2] = \\text{Var}[e_t] + \\theta^2 \\text{Var}[e_{t-1}^2] = \\sigma_e^2 + \\theta^2 \\text{E}[e_{t-1}^4] = \\sigma_e^2 + 3 \\theta^2 \\sigma_e^4 $$\n",
    "\n",
    "where we used the fact that the fourth moment of a normal distribution $N(0, \\sigma^2)$ is $\\text{E}[X^4] = 3 \\sigma^4$.\n",
    "\n",
    "The autocovariance of $\\{ Y_t \\}$ for lag $k > 1$ is\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\text{Cov}[Y_t, Y_{t - k}] &= \\text{Cov}[e_t - \\theta (e_{t - 1})^2, e_{t - k} - \\theta (e_{t - k - 1})^2] \\\\\n",
    "&= \\text{Cov}[e_t, e_{t - k}] - \\theta \\text{Cov}[e_t, (e_{t - k - 1})^2] - \\theta \\text{Cov}[(e_{t - 1})^2, e_{t - k}] + \\theta^2 \\text{Cov}[(e_{t - 1})^2, (e_{t - k - 1})^2] \\\\\n",
    "&= 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "since all random variables within the covariances are independent.\n",
    "\n",
    "The autocovariance of $\\{ Y_t \\}$ for lag $k = 1$ is\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\text{Cov}[Y_t, Y_{t - 1}] &= \\text{Cov}[e_t - \\theta (e_{t - 1})^2, e_{t - 1} - \\theta (e_{t - 2})^2] \\\\\n",
    "&= \\text{Cov}[e_t, e_{t - 1}] - \\theta \\text{Cov}[e_t, (e_{t -2})^2] - \\theta \\text{Cov}[(e_{t - 1})^2, e_{t - 1}] + \\theta^2 \\text{Cov}[(e_{t - 1})^2, (e_{t - 2})^2] \\\\\n",
    "&= - \\theta \\text{Cov}[(e_{t - 1})^2, e_{t - 1}] \\\\\n",
    "&= -\\theta \\left( \\text{E}[e_{t - 1}^3] - \\text{E}[e_{t - 1}^2] \\text{E}[e_{t - 1}]\\right) \\\\\n",
    "&= 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where we used the fact that the third moment of a normal distribution  $N(0, \\sigma^2)$ is $\\text{E}[X^3] = 0$ and the first moment of the distribution is its mean, which is also zero.\n",
    "\n",
    "Therefore, the autocovariance of $\\{Y_t\\}$ is\n",
    "\n",
    "$$\n",
    "\\gamma_k = \\begin{cases}\n",
    "\\sigma_e^2 + 3 \\theta^2 \\sigma_e^4 &\\text{for } k = 0 \\\\\n",
    "0 &\\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "and the autocorrelation is\n",
    "\n",
    "$$\n",
    "\\rho_k = \\begin{cases}\n",
    "1 &\\text{for } k = 0 \\\\\n",
    "0 &\\text{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  Yes.  The series has a constant mean $\\overline{Y} = \\theta \\sigma_e^2$, and an autocovariance which does not depend on $t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.14**.  Evaluate the mean and the covariance function for each of the following processes.  In each case, determine whether or not the process is stationary.\n",
    "\n",
    "**(a)**  $Y_t = \\theta_0 + t e_t$.\n",
    "\n",
    "**(b)**  $W_t = \\nabla Y_t$, where $Y_t$ is given in part (a).\n",
    "\n",
    "**(c)**  $Y_t = e_t e_{t - 1}$.  (You may assume that $\\{e_t\\}$ is normal white noise.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  The mean of the process is\n",
    "\n",
    "$$ \\text{E}[Y_t] = \\text{E}[\\theta_0 + t e_t] = \\theta_0 + t \\text{E}[e_t] = \\theta_0 $$\n",
    "\n",
    "The variance of the process is\n",
    "\n",
    "$$ \\text{Var}[Y_t] = \\text{Var}[\\theta_0 + t e_t] = t^2 \\text{Var}[e_t] = t^2 \\sigma_e^2 $$\n",
    "\n",
    "which is a function of time, and so the process cannot be stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  The mean of the process is\n",
    "\n",
    "$$ \\text{E}[\\nabla Y_t] = \\text{E}[Y_t - Y_{t - 1}] = \\text{E}[Y_t] - \\text{E}[Y_{t - 1}] = \\theta_0 - \\theta_0 = 0 $$\n",
    "\n",
    "The variance of the process is\n",
    "\n",
    "$$ \\text{Var}[\\nabla Y_t] = \\text{Var}[Y_t - Y_{t - 1}] = \\text{Var}[Y_t] + \\text{Var}[Y_{t - 1}] = t^2 \\sigma_e^2 + (t - 1)^2 \\sigma_e^2 = (2t^2 + 2t - 1) \\sigma_e^2 $$\n",
    "\n",
    "which is a function of time, and so the process cannot be stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)**  The mean of the process is\n",
    "\n",
    "$$ \\text{E}[Y_t] = \\text{E}[e_t e_{t-1}] = \\text{E}[e_t] \\text{E}[e_{t - 1}] = 0 $$\n",
    "\n",
    "The variance of the process is\n",
    "\n",
    "$$ \\text{Var}[Y_t] = \\text{Var}[e_t e_{t - 1}] = \\text{E}[e_t^2 e_{t - 1}^2] = \\text{E}[e_t^2] \\text{E}[ e_{t - 1}^2] = \\sigma_e^2 $$\n",
    "\n",
    "The autocorrelation of the process with lag $k > 0$ is\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\text{Cov}[Y_t, Y_{t - k}] &= \\text{E}[Y_t Y_{t - k}] - \\text{E}[Y_t] \\text{E}[Y_{t - k}]  \\\\\n",
    "&= \\text{E}[e_t e_{t - 1} e_{t - k} e_{t - k - 1}] - 0 \\cdot 0 \\\\\n",
    "&= \\text{E}[e_t] \\text{E}[e_{t - 1} e_{t - k} e_{t - k - 1}] \\\\\n",
    "&= 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Therefore, the autocovariance of the process is\n",
    "\n",
    "$$ \n",
    "\\gamma_k = \\begin{cases}\n",
    "\\sigma_e^2 &\\text{for } k = 0 \\\\\n",
    "0 &\\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Since the process has a constant mean and autocovariance that is free of time $t$, the process is stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.15**.  Suppose that $X$ is a random variable with zero mean.  Define a time series by $Y_t = (-1)^t X$.\n",
    "\n",
    "**(a)** Find the mean function for $\\{ Y_t \\}$.\n",
    "\n",
    "**(b)** Find the covariance function for $\\{ Y_t \\}$.\n",
    "\n",
    "**(c)** Is $\\{Y_t\\}$ stationary?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  The mean function is\n",
    "\n",
    "$$ \\text{E}[Y_t] = \\text{E}[(-1)^t X] = (-1)^t \\text{E}[X] = (-1)^t \\cdot 0 = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  The variance is\n",
    "\n",
    "$$ \\text{Var}[Y_t] = \\text{E}[Y_t^2] - \\text{E}[Y_t]^2 = \\text{E}[(-1)^{2t}X^2]  - 0^2= \\text{E}[X^2] = \\text{Var}[X] = \\sigma_X^2 $$\n",
    "\n",
    "The covariance for even lag $k$ is\n",
    "\n",
    "$$ \\text{Cov}[Y_t, Y_{t - k}] = \\text{Cov}[Y_t, Y_t] = \\text{Var}[Y_t] = \\sigma_X^2 $$\n",
    "\n",
    "and the covariance for odd lag $k$ is\n",
    "\n",
    "$$ \\text{Cov}[Y_t, Y_{t - k}] = \\text{Cov}[Y_t, -Y_t] = -\\text{Var}[Y_t] = -\\sigma_X^2 $$\n",
    "\n",
    "Therefore the covariance function is\n",
    "\n",
    "$$ \n",
    "\\gamma_{t, s} = \\begin{cases}\n",
    "\\sigma_X^2 &\\text{for } |t - s| \\text{ even} \\\\\n",
    "-\\sigma_X^2 &\\text{for } |t - s| \\text{ odd}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)**  Yes.  The mean is constant, and the autocovariance function is $t$ free, depending only on the lag:\n",
    "\n",
    "$$ \\gamma_k = \\begin{cases}\n",
    "\\sigma_X^2 &\\text{for } k \\text{ even} \\\\\n",
    "-\\sigma_X^2 &\\text{for } k \\text{ odd} \\\\\n",
    "\\end{cases} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.16**.  Suppose $Y_t = A + X_t$, where $\\{X_t\\}$ is stationary and $A$ is random but independent of $\\{ X_t \\}$.  Find the mean and covariance function for $\\{Y_t\\}$ in terms of the mean and autocovariance function for $\\{ X_t \\}$ and the mean and variance of $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**.\n",
    "\n",
    "The mean is\n",
    "\n",
    "$$ \\text{E}[Y_t] = \\text{E}[A + X_t] = \\text{E}[A] + \\text{E}[X_t] = \\mu_A + \\mu_X $$\n",
    "\n",
    "where $\\mu_A, \\mu_X$ are the means of $A$ and $\\{X_t\\}$.\n",
    "\n",
    "The variance is\n",
    "\n",
    "$$ \\text{Var}[Y_t] = \\text{Var}[A + X_t] = \\text{Var}[A] + \\text{Var}[X_t] = \\sigma_A^2 + \\sigma_X^2 $$\n",
    "\n",
    "The covariance is\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\text{Cov}[Y_t, Y_s] &= \\text{Cov}[A + X_t, A + X_s]\n",
    "&= \\text{Cov}[A, A] + \\text{Cov}[A, X_s] + \\text{Cov}[X_t, A] + \\text{Cov}[X_t, X_s] \\\\\n",
    "&= \\sigma_A^2 + \\gamma_{t, s}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Therefore, the autocovariance function for $\\{Y_t\\}$ is\n",
    "\n",
    "$$ \\omega_k = \\sigma_A^2 + \\gamma_k $$\n",
    "\n",
    "where $\\gamma_k$ is the autocovariance function for $\\{X_t\\}$ and $\\sigma_A^2$ is the variance of $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.17**.  Let $\\{ Y_t \\}$ be stationary with autocovariance function $\\gamma_k$.  Let $\\overline{Y} = \\frac{1}{n} \\sum_{t=1}^n Y_t$.  Show that\n",
    "\n",
    "$$ \\text{Var}[\\overline{Y}] = \\frac{\\gamma_0}{n} + \\frac{2}{n} \\sum_{k=1}^{n -1} \\left( 1 - \\frac{k}{n} \\right) \\gamma_k \n",
    "= \\frac{1}{n} \\sum_{k=-n+1}^{n - 1} \\left(1 - \\frac{|k|}{n} \\right) \\gamma_k\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**.  We have\n",
    "\n",
    "$$ \n",
    "\\text{Var}[\\overline{Y}] = \\text{Var}\\left[ \\frac{1}{n} \\sum_{t=1}^n Y_i \\right] = \\frac{1}{n^2} \\sum_{i=1}^n \\sum_{j=1}^n \\text{Cov}[Y_i, Y_j] = \\frac{1}{n^2} \\sum_{i=1}^n \\sum_{j=1}^n \\gamma_{|j - i|}\n",
    "$$\n",
    "\n",
    "The term with lag $k = |j - i|$ appears $2(n - k)$ times, as that is the number of ways of selecting $i, j$ in the summation intervals to produce that lag.  The equalities in the result follow as a consequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.18**.  Let $\\{ Y_t \\}$ be stationary with autocovariance function $\\gamma_k$.  Define the sample variance as $ S^2 = \\frac{1}{n - 1} \\sum_{t=1}^n (Y_t - \\overline{Y})^2$.\n",
    "\n",
    "**(a)** First show that $\\sum_{t=1}^n (Y_t - \\mu)^2 = \\sum_{t = 1}^n (Y_t - \\overline{Y})^2 + n(\\overline{Y} - \\mu)^2$.\n",
    "\n",
    "**(b)** Use part (a) to show that\n",
    "\n",
    "**(c)**  $$ \\text{E}[S^2] = \\frac{n}{n - 1} \\gamma_0 - \\frac{n}{n - 1} \\text{Var}[\\overline{Y}] = \\gamma_0 - \\frac{2}{n - 1} \\sum_{k=1}^{n - 1} \\left( 1 - \\frac{n}{k} \\right) \\gamma_k $$\n",
    "\n",
    "(Use the results of Exercise 2.17 for the last expression.)\n",
    "\n",
    "**(d)** It $\\{Y_t \\}$ is a white noise process with variance $\\gamma_0$, show that $\\text{E}[S^2] = \\gamma_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  We have\n",
    "\n",
    "$$ \\sum_{t=1}^n (Y_t - \\mu)^2 =  \\sum_{t=1}^n \\left((Y_t - \\overline{Y}) + (\\overline{Y} - \\mu)\\right)^2 = \\sum_{t=1}^n (Y_t - \\overline{Y})^2 + \\sum_{t=1}^n (\\overline{Y} - \\mu)^2 + 2 \\sum_{t=1}^n (Y_t - \\overline{Y})(\\overline{Y} - \\mu) $$\n",
    "\n",
    "But the last term is\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "2 \\sum_{t=1}^n (Y_t - \\overline{Y})(\\overline{Y} - \\mu) &= 2 (\\overline{Y} - \\mu) \\sum_{t=1}^n (Y_t - \\overline{Y}) \\\\\n",
    "&= 2 (\\overline{Y} - \\mu) \\left( \\left( \\sum_{t=1}^n Y_t \\right) - n \\overline{Y} \\right) \\\\\n",
    "&= 2 (\\overline{Y} - \\mu) ( n \\overline{Y} - n \\overline{Y} ) \\\\\n",
    "&= 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "and so the result follows,\n",
    "\n",
    "$$ \\sum_{t=1}^n (Y_t - \\mu)^2 = \\sum_{t=1}^n (Y_t - \\overline{Y})^2 + n (\\overline{Y} - \\mu)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b, c)** We have\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{E}[S^2] &= \\frac{1}{n - 1} \\text{E} \\left[ \\sum_{t=1}^n (Y_t - \\overline{Y})^2 \\right] \\\\\n",
    "&= \\frac{1}{n - 1} \\text{E} \\left[\\sum_{t=1}^n (Y_t - \\mu)^2 - n (\\overline{Y} - \\mu)^2 \\right] \\\\\n",
    "&= \\frac{1}{n - 1} \\left( \\text{E} \\left[\\sum_{t=1}^n (Y_t - \\mu)^2 \\right] - n (\\overline{Y} - \\mu)^2 \\right) \\\\\n",
    "&= \\frac{1}{n - 1} \\left( \\text{Var}[Y_t] - n \\text{Var}[\\overline{Y}] \\right) \\\\\n",
    "&= \\frac{n}{n - 1} \\gamma_0 - \\frac{n}{n - 1} \\text{Var}[\\overline{Y}]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "and the second equality follows directly by replacing $\\text{Var}[\\overline{Y}]$ with the expression in the result of Exercise 2.17."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)**  If $\\{Y_t\\}$ is a white noise process with variance $\\gamma_0$, the autocovariance function is 0 for any positive lags.  Replacing these in the second expression for $\\text{E}[S^2]$ in the previous item, we get $\\text{E}[S^2] = \\gamma_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.19**.  Let $Y_1 = \\theta_0 + e_1$, and then for $t > 1$ define $Y_t$ recursively by $Y_t = \\theta_0 + Y_{t - 1} + e_t$.  Here $\\theta_0$ is a constant.  The process $\\{Y_t\\}$ is called a **random walk with drift**.\n",
    "\n",
    "**(a)** Show that $Y_t$ may be rewritten as $Y_t = t\\theta_0 + e_t + e_{t - 1} + \\cdots + e_1 $.\n",
    "\n",
    "**(b)** Find the mean function for $Y_t$.\n",
    "\n",
    "**(c)** Find the autocovariance function for $Y_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  The result is true for $t = 1$.  Assuming by induction it is valid for $t - 1$,\n",
    "\n",
    "$$ Y_t = \\theta_0 + Y_{t - 1} + e_t = \\theta_0 + (t - 1)\\theta_0 + \\sum_{i=1}^{t - 1} e_i + e_t = t \\theta_0 + \\sum_{i=1}^t e_i $$\n",
    "\n",
    "and so the result holds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  The mean function is\n",
    "\n",
    "$$ \\text{E}[Y_t] = \\text{E}\\left[ t \\theta_0 + \\sum_{i=1}^t e_i \\right] = t \\theta_0 + \\sum_{i=1}^t \\text{E}[e_i] = t \\theta_0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)**  The variance is\n",
    "\n",
    "$$ \\text{Var}[Y_t] = \\text{Var}\\left[t \\theta_0 + \\sum_{i=1}^t e_i\\right] = \\sum_{i=1}^t \\text{Var}[e_i] = t \\sigma_e^2 $$\n",
    "\n",
    "and the autocovariance for lag $k > 0$ is\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\text{Cov}[Y_t, Y_{t - k}] &= \\text{Cov}\\left[t\\theta_0 + \\sum_{i=1}^t e_i, (t - k)\\theta_0 + \\sum_{j=1}^{t - k} e_j \\right]  \\\\\n",
    "&= \\sum_{i=1}^t \\sum_{j=1}^{t - k} \\text{Cov}[e_i, e_j]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The covariance terms above are non-zero only for $i = j$, and so\n",
    "\n",
    "$$ \\text{Cov}[Y_t, Y_{t - k}] = (t - k) \\sigma_e^2 $$\n",
    "\n",
    "Therefore the autocovariance function is\n",
    "\n",
    "$$ \\gamma_{t, s} = t \\sigma_e^2 \\quad \\text{for } 1 \\leq t \\leq s $$\n",
    "\n",
    "This is exactly the same as the autocovariance function for the random walk, but now the mean is not constant -- it has a drift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.20**.  Consider the standard random walk model where $Y_t = Y_{t - 1} + e_t$ with $Y_1 = e_t$.\n",
    "\n",
    "**(a)** Use the representation of $Y_t$ above to show that $\\mu_t = \\mu_{t - 1}$ for $t > 1$ with initial condition $\\mu_1 = \\text{E}[e_1] = 0$.  Hence show that $\\mu_t = 0$ for all $t$.\n",
    "\n",
    "**(b)** Similarly, show that $\\text{Var}[Y_t] = \\text{Var}[Y_{t - 1}] + \\sigma_e^2$ for $t > 1$ with $\\text{Var}[Y_1] = \\sigma-e^2$ and hence $\\text{Var}[Y_t] = t \\sigma_e^2$.\n",
    "\n",
    "**(c)** For $0 \\leq t \\leq s$, use $Y_s = Y_t + e_{t + 1} + e_{t + 2} + \\cdots + e_s$ to show that $\\text{Cov}[Y_t, Y_s] = \\text{Var}[Y_t]$ and, hence, that $\\text{Cov}[Y_t, Y_s] = \\min(t, s) \\sigma_e^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  By induction,\n",
    "\n",
    "$$\\mu_t = \\text{E}[Y_t] = \\text{E}[Y_{t - 1} + e_t] = \\text{E}[Y_{t - 1}] + \\text{E}[e_t] = \\mu_{t - 1} + 0 = \\mu_{t - 1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  By induction,\n",
    "\n",
    "$$ \\text{Var}[Y_t] = \\text{Var}[Y_{t - 1} + e_t] = \\text{Var}[Y_{t - 1}] + \\text{Var}[e_t] = (t - 1)\\sigma_e^2 + \\sigma_e^2 = t \\sigma_ e^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)**  Assuming without loss of generality that $t \\leq s$,\n",
    "\n",
    "$$ \\text{Cov}[Y_t, Y_s] = \\text{Cov}\\left[ Y_t, Y_t + \\sum_{j=t+1}^s e_j \\right] = \\text{Cov}[Y_t, Y_t] + \\sum_{j=t+1}^s \\text{Cov}[Y_t, e_j] = \\text{Var}[Y_t] + 0 = t \\sigma_e^2$$\n",
    "\n",
    "For the case of $t > s$, we can do $\\text{Cov}[Y_t, Y_s] = \\text{Cov}[Y_s, Y_t] = s \\sigma_e^2$.\n",
    "\n",
    "Therefore, $\\text{Cov}[Y_t, Y_s] = \\min(t, s) \\sigma_e^2$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
