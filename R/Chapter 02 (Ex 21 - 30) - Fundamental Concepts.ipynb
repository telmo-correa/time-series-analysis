{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.21**.  For a random walk with random starting value, let $Y_t = Y_0 + e_t + e_{t - 1} + \\cdots + e_1$ for $t > 0$, where $Y_0$ has a distribution with mean $\\mu_0$ and variance $\\sigma_0^2$.  Suppose further that $Y_0, e_1, \\dots, e_t$ are independent.\n",
    "\n",
    "**(a)** Show that $\\text{E}[Y_t] = \\mu_0$ for all $t$.\n",
    "\n",
    "**(b)** Show that $\\text{Var}[Y_t] = t \\sigma_e^2 + \\sigma_0^2$.\n",
    "\n",
    "**(c)** Show that $\\text{Cov}[Y_t, Y_s] = \\min(t, s) \\sigma_e^2 + \\sigma_0^2$.\n",
    "\n",
    "**(d)** Show that \n",
    "\n",
    "$$ \\text{Corr}[Y_t, Y_s] = \\sqrt{\\frac{t \\sigma_e^2 + \\sigma_0^2}{s \\sigma_e^2 + \\sigma_0^2}} \\quad \\text{for } 0 \\leq t \\leq s $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**\n",
    "\n",
    "$$ \\text{E}[Y_t] = \\text{E}\\left[ Y_0 + \\sum_{i=1}^t e_i \\right] = \\text{E}[Y_0] + \\sum_{i=1}^t \\text{E}[e_i] = \\mu_0 + 0 = \\mu_0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**\n",
    "\n",
    "$$ \\text{Var}[Y_t] = \\text{Var}\\left[ Y_0 + \\sum_{i=1}^t e_i \\right] = \\text{Var}[Y_0] + \\sum_{i=1}^t \\text{Var}[e_i] = \\sigma_0^2 + t\\sigma_e^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)**  Assuming without loss of generality that $t \\leq s$,\n",
    "\n",
    "$$ \\text{Cov}[Y_t, Y_s] = \\text{Cov}\\left[ Y_t, Y_t + \\sum_{j=t+1}^s e_j \\right] = \\text{Cov}[Y_t, Y_t] + \\sum_{j=t+1}^s \\text{Cov}[Y_t, e_j] = \\text{Var}[Y_t] = t\\sigma_e^2 + \\sigma_0^2 $$\n",
    "\n",
    "For the case where $s > t$, we can do $\\text{Cov}[Y_t, Y_s] = \\text{Cov}[Y_s, Y_t] = s \\sigma_e^2 + \\sigma_0^2$.\n",
    "\n",
    "Therefore, $\\text{Cov}[Y_t, Y_s] = \\min(t, s) \\sigma_e^2 + \\sigma_0^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)**  Assuming $0 \\leq t \\leq s$, we have\n",
    "\n",
    "$$ \\text{Corr}[Y_t, Y_s] = \\frac{\\text{Cov}[Y_t, Y_s]}{\\sqrt{\\text{Var}[Y_t] \\text{Var}[Y_s]}} = \n",
    "\\frac{t \\sigma_e^2 + \\sigma_0^2}{\\sqrt{(t \\sigma_e^2 + \\sigma_0^2)(s \\sigma_e^2 + \\sigma_0^2)}}\n",
    "= \\sqrt{\\frac{t \\sigma_e^2 + \\sigma_0^2}{s \\sigma_e^2 + \\sigma_0^2}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.22**.  Let $\\{e_t\\}$ be a zero-mean white noise process, and let $c$ be a constant with $|c| < 1$.  Define $Y_t$ recursively by $Y_t = c Y_{t - 1} + e_t$ with $Y_1 = e_1$.\n",
    "\n",
    "**(a)** Show that $\\text{E}[Y_t] = 0$.\n",
    "\n",
    "**(b)** Show that $\\text{Var}[Y_t] = \\sigma_e^2 (1 + c^2 + c^4 + \\cdots + c^{2t - 2})$.  Is $\\{Y_t\\}$ stationary?\n",
    "\n",
    "**(c)** Show that\n",
    "\n",
    "$$ \\text{Corr}[Y_t, Y_{t-1}] = c \\sqrt{\\frac{\\text{Var}[Y_{t - 1}]}{\\text{Var}[Y_t]}} $$\n",
    "\n",
    "and, in general,\n",
    "\n",
    "$$ \\text{Corr}[Y_t, Y_{t-k}] = c^k \\sqrt{\\frac{\\text{Var}[Y_{t - k}]}{\\text{Var}[Y_t]}} $$\n",
    "\n",
    "Hint:  Argue that $Y_{t - 1}$ is independent of $e_t$.  Then, use\n",
    "\n",
    "$$ \\text{Cov}[Y_t, Y_{t - 1}] = \\text{Cov}[cY_{t - 1} + e_t, Y_{t - 1}] $$\n",
    "\n",
    "**(d)**  For large $t$, argue that\n",
    "\n",
    "$$ \\text{Var}[Y_t] \\approx \\frac{\\sigma_e^2}{1 - c^2} \n",
    "\\quad \\text{and} \\quad\n",
    "\\text{Corr}[Y_t, Y_{t - k}] \\approx c^k\n",
    "\\quad \\text{for } k > 0 $$\n",
    "\n",
    "so that $\\{ Y_t \\}$ could be called **asymptotically stationary**.\n",
    "\n",
    "**(e)** Suppose now that we alter the initial condition and put $Y_1 = \\frac{e_1}{\\sqrt{1 - c^2}}$.  Show that now $\\{Y_t\\}$ is stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  We have $\\text{E}[Y_1] = \\text{E}[e_1] = 0$.  By induction,\n",
    "\n",
    "$$ \\text{E}[Y_t] = \\text{E}[cY_{t - 1} + e_t] = c \\text{E}[Y_{t - 1}] + \\text{E}[e_t] = c \\cdot 0 + 0 = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  We have $\\text{Var}[Y_1] = \\text{Var}[e_1] = \\sigma_e^2$.  By induction,\n",
    "\n",
    "$$ \\text{Var}[Y_t] = \\text{Var}[c Y_{t - 1} + e_t] = c^2 \\text{Var}[Y_{t - 1}] + \\text{Var}[e_t] = c^2 \\left( \\sigma_e^2 \\sum_{j = 0}^{t - 2} c^{2j} \\right) + \\sigma_e^2 = \\sigma_e^2 \\sum_{j=0}^{t - 1} c^{2j} $$\n",
    "\n",
    "Since this depends on the value of $t$, the autocovariance is dependent on $t$ for lag 0, and so $\\{Y_t\\}$ is not stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)**  We have:\n",
    "\n",
    "$$ \\text{Cov}[Y_t, Y_{t - 1}] = \\text{Cov}[c Y_{t - 1} + e_t, Y_{t - 1}] = c \\text{Var}[Y_{t - 1}] $$\n",
    "\n",
    "and so\n",
    "\n",
    "$$ \\text{Corr}[Y_t, Y_{t - 1}] = \\frac{ c \\text{Var}[Y_{t - 1}]}{ \\sqrt{\\text{Var}[Y_t]\\text{Var}[Y_{t - 1}]} } = c \\sqrt{\\frac{\\text{Var}[Y_{t - 1}]}{\\text{Var}[Y_t]}} $$\n",
    "\n",
    "In general\n",
    "\n",
    "$$ \\text{Cov}[Y_t, Y_{t - k} = \\text{Cov}[c Y_{t - 1} + e_t, Y_{t - k}] = c \\text{Cov}[Y_{t - 1}, Y_{t - k}] = \\cdots = c^k \\text{Cov}[Y_{t - k}, Y_{t - k}] = c^k \\text{Var}[Y_{t - k}] $$\n",
    "\n",
    "and so\n",
    "\n",
    "$$ \\text{Corr}[Y_t, Y_{t - k}] = \\frac{ c^k \\text{Var}[Y_{t - k}]}{ \\sqrt{\\text{Var}[Y_t]\\text{Var}[Y_{t - k}]} } = c^k \\sqrt{\\frac{\\text{Var}[Y_{t - k}]}{\\text{Var}[Y_t]}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)**  For large $t$ -- or more precisely, on the limit as $t \\rightarrow \\infty$, the following geometric sum is\n",
    "\n",
    "$$ \\lim_{t \\rightarrow \\infty} \\sum_{j = 0}^{t - 2} c^{2j} = \\frac{1}{1 - c^2} $$\n",
    "\n",
    "and so\n",
    "\n",
    "$$ \\lim_{t \\rightarrow \\infty} \\text{Var}[Y_t] = \\frac{\\sigma_e^2}{1 - c^2} $$\n",
    "\n",
    "and\n",
    "\n",
    "$$ \\lim_{t \\rightarrow \\infty} \\text{Corr}[Y_t, Y_{t - k}] = \\lim_{t \\rightarrow \\infty} c^k \\sqrt{\\frac{Y_{t - k}}{Y_t}} = c^k $$\n",
    "\n",
    "These values at the limit do not depend on $t$, which aligns with the definition of $\\{Y_t\\}$ being asymptotically stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e)** The mean of the new initial value is still 0, so the previous results on the mean being 0 still hold.\n",
    "\n",
    "The variance now becomes\n",
    "\n",
    "$$ \\text{Var}[Y_1] = \\frac{1}{1 - c^2} \\text{Var}[e_1] = \\frac{\\sigma_e^2}{1 - c^2} $$\n",
    "\n",
    "and the autocovariance becomes, by induction,\n",
    "\n",
    "$$ \\text{Cov}[Y_t, Y_{t - k}] = \\text{Cov}[c Y_{t - 1} + e_t, Y_{t - k}] = c \\text{Cov}[Y_{t - 1}, Y_{t - k}] = c \\left( \\frac{c^{k-1}}{1 - c^2} \\right) = \\frac{c^k}{1 - c^2} $$\n",
    "\n",
    "The conditions for stationarity are now satisfied, since the mean is still constant and the autocorrelation function is free of $t$, instead depending on the lag,\n",
    "\n",
    "$$ \\gamma_k = \\frac{c^k}{1 - c^2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.23**. Two processes $\\{Z_t\\}$ and $\\{Y_t\\}$ are said to be **independent** if for any time points $t_1, t_2, \\dots, t_m$ and $s_1, s_2, \\dots, s_n$ the random variables $\\{Z_{t_1}, Z_{t_2}, \\dots, Z_{t_m}\\}$ are independent of the random variables $\\{ S_{s_1}, S_{s_2}, \\dots, S_{s_n}\\}$.  Show that if $\\{Z_t\\}$ and $\\{Y_t\\}$ are independent stationary processes, then $W_t = Z_t + Y_t$ is stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**.  We have:\n",
    "\n",
    "$$ \\text{E}[W_t] = \\text{E}[Z_t] + \\text{E}[Y_t] = \\mu_Z + \\mu_Y $$\n",
    "\n",
    "and so the mean of $\\{W_t\\}$ does not depend on time.\n",
    "\n",
    "Let $\\alpha_k$ be the autocorrelation function of the $\\{Z_t\\}$ and $\\beta_k$ be the autocorrelation function of the $\\{Y_t\\}$.  We have\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{Cov}[W_t, W_{t - k}] &= \\text{Cov}[Z_t + Y_t, Z_{t - k} + Y_{t - k}] \\\\\n",
    "&= \\text{Cov}[Z_t, Z_{t - k}] + \\text{Cov}[Z_t, Y_{t - k}] + \\text{Cov}[Z_{t - k}, Y_t] + \\text{Cov}[Y_t. Y_{t - k}] \\\\\n",
    "&= \\alpha_k + \\beta_k\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "since $\\text{Cov}[Z_t, Y_{t - k}] = \\text{Cov}[Z_{t - k}, Y_t]$ from the independence assumption.\n",
    "\n",
    "Therefore, the autocorrelation function of $\\{W_t\\}$ is free of $t$, and since the mean of $\\{W_t\\}$ is constant $\\{W_t\\}$ is stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.24**.  Let $\\{X_t\\}$ be a time series in which we are interested.  However, because the measurement process itself is not perfect, we actually observe $Y_t = X_t + e_t$.  We assume that $\\{X_t\\}$ and $\\{e_t\\}$ are independent processes.  We call $X_t$ the **signal** and $e_t$ the **measurement noise** or **error process**.\n",
    "\n",
    "If $\\{X_t\\}$ is stationary with autocorrelation function $\\rho_k$, show that $\\{Y_t\\}$ is also stationary with\n",
    "\n",
    "$$ \\text{Corr}[Y_t, Y_{t - k}] = \\frac{\\rho_k}{1 + \\sigma_e^2 / \\sigma_X^2} \\quad \\text{for } k \\geq 1 $$\n",
    "\n",
    "We call $\\sigma_X^2 / \\sigma_e^2$ the **signal-to-noise ratio**, or SNR.  Note that the larger the SNR, the closer the autocorrelation function of the observed process $\\{Y_t\\}$ is to the autocorrelation function of the desired signal $\\{X_t\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**. The variance of the measured signal is\n",
    "\n",
    "$$ \\text{Var}[Y_t] = \\text{Var}[X_t + e_t] = \\text{Var}[X_t] + \\text{Var}[e_t] = \\sigma_X^2 + \\sigma_e^2 $$\n",
    "\n",
    "We have, for $k \\geq 1$,\n",
    "\n",
    "$$ \\text{Cov}[Y_t, Y_{t - k}] = \\text{Cov}[X_t + e_t, X_{t - k} + e_{t - k}] = \\text{Cov}[X_t, X_{t - k}] = \\sigma_X^2 \\rho_k $$\n",
    "\n",
    "where we have used independence to cancel out the covariance between $X_a$ and $e_b$ and the white noise property to cancel out $\\text{Cov}[e_t, e_{t - k}]$.\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$$ \\text{Corr}[Y_t, Y_{t - k}] = \\frac{\\text{Cov}[Y_t, Y_{t - k}]}{\\sqrt{\\text{Var}[Y_t]\\text{Var}[Y_{t - k}]}} = \\frac{\\sigma_X^2 \\rho_k}{\\sigma_X^2 + \\sigma_e^2} = \\frac{\\rho_k}{1 + \\sigma_e^2 / \\sigma_X^2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.25**.  Suppose $Y_t = \\beta_0 + \\sum_{i=1}^k [A_i \\cos(2 \\pi f_i t) + B_i \\sin(2 \\pi f_i t)] $, where $\\beta_0, f_1, f_2, \\dots, f_k$ are constants and $A_1, A_2, \\dots, A_k, B_1, B_2, \\dots, B_k$ are independent random variables with zero means and variances $\\text{Var}[A_i] = \\text{Var}[B_i] = \\sigma_i^2$. Show that $\\{Y_i\\}$ is stationary and find its covariance function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**.  We have mean\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\text{E}[Y_t] &= \\text{E}\\left[\\beta_0 + \\sum_{i=1}^k [A_i \\cos(2 \\pi f_i t) + B_i \\sin(2 \\pi f_i t)] \\right] \\\\\n",
    "&= \\beta_0 + \\sum_{i=1}^k \\left( \\text{E}[A_i] \\cos(2 \\pi f_i t)  + \\text{E}[B_i] \\sin(2 \\pi f_i t) \\right) \\\\\n",
    "&= \\beta_0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The variance is\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{Var}[Y_t] &= \\text{Var}\\left[\\beta_0 + \\sum_{i=1}^k [A_i \\cos(2 \\pi f_i t) + B_i \\sin(2 \\pi f_i t)] \\right] \\\\\n",
    "&= \\sum_{i=1}^k \\left (\\text{Var}[A_i] \\cos^2(2 \\pi f_i t) + \\text{Var}[B_i] \\sin^2(2 \\pi f_i t) \\right) \\\\\n",
    "&= \\sum_{i=1}^k \\sigma_i^2 \\left(\\cos^2(2 \\pi f_i t) + \\sin^2(2 \\pi f_i t) \\right) \\\\\n",
    "&= \\sum_{i=1}^k \\sigma_i^2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where we used the trigonometric identity $\\cos^2 \\alpha + \\sin^2 \\alpha = 1$.\n",
    "\n",
    "The autocovariance for lag $k > 0$ is\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{Cov}[Y_t, Y_{t - k}] &= \\text{Cov}\\left[\\beta_0 + \\sum_{i=1}^k [A_i \\cos(2 \\pi f_i t) + B_i \\sin(2 \\pi f_i t)], \\beta_0 + \\sum_{i=1}^k [A_i \\cos(2 \\pi f_i (t - k)) + B_i \\sin(2 \\pi f_i (t - k))] \\right] \\\\\n",
    "&= \\sum_{i=1}^k \\sum_{j=1}^k \\text{Cov}[A_i \\cos(2 \\pi f_i t) + B_i \\sin(2 \\pi f_i t), A_j \\cos(2 \\pi f_i (t - k)) + B_j \\sin(2 \\pi f_i (t - k))] \\\\\n",
    "&= \\sum_{i=1}^k \\cos(2 \\pi f_i t) \\cos(2 \\pi f_i (t-k)) \\text{Var}[A_i] + \\sin(2 \\pi f_i t) \\sin(2 \\pi f_i (t-k)) \\text{Var}[B_i]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where, since the $A_i$'s and $B_i$'s are all independent, we cancel out all covariance terms that do not repeat the same random variable on both sides.\n",
    "\n",
    "Continuing the calculation, and using the trigonometric identify $\\cos (\\alpha - \\beta) = \\cos \\alpha \\cos \\beta + \\sin \\alpha \\sin \\beta$,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{Cov}[Y_t, Y_{t - k}]| \n",
    "&= \\sum_{i=1}^k \\sigma_i^2 \\left(\\cos(2 \\pi f_i t) \\cos(2 \\pi f_i (t-k)) + \\sin(2 \\pi f_i t) \\sin(2 \\pi f_i (t-k)) \\right) \\\\\n",
    "&= \\sum_{i=1}^k \\sigma_i^2 \\cos(2 \\pi f_i t - 2 \\pi f_i (t-k)) \\\\\n",
    "&= \\sum_{i=1}^k \\sigma_i^2 \\cos(2 \\pi f_i k)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "which is free of $t$.  The autocovariance function is, more explicitly,\n",
    "\n",
    "$$ \\gamma_k = \\sum_{i=1}^k \\sigma_i^2 \\cos(2 \\pi f_i k) $$\n",
    "\n",
    "and we have shown that $\\{Y_i\\}$ is stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.26**.  Define the function $\\Gamma_{t, s} = \\frac{1}{2} \\text{E}[(Y_t - Y_s)^2]$.  In geostatistics, $\\Gamma_{t, s}$ is called the **semivariogram**.\n",
    "\n",
    "**(a)** Show that for a stationary process $\\Gamma_{t, s} = \\gamma_0 - \\gamma_{|t - s|}$.\n",
    "\n",
    "**(b)** A process is said to be **intrinsically stationary** if $\\Gamma_{t, s}$ depends only on the time difference $|t - s|$.  Show that the random walk process is intrinsically stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** For a stationary process,\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\Gamma_{t, s} &= \\frac{1}{2} \\text{E}[(Y_t - Y_s)^2] = \\frac{1}{2} \\text{E}[Y_t^2 + Y_s^2 - 2 Y_t Y_s] \\\\\n",
    "&= \\frac{1}{2} \\left( \\text{E}[Y_t^2] + \\text{E}[Y_s^2] - 2 \\text{E}[Y_t Y_s] \\right) \\\\\n",
    "&= \\frac{1}{2} \\left( \\text{Var}[Y_t] + \\text{E}[Y_t]^2 + \\text{Var}[Y_s] + \\text{E}[Y_s]^2 - 2 \\left( \\text{Cov}[Y_t, Y_s] + \\text{E}[Y_t] E[Y_s] \\right) \\right) \\\\\n",
    "&= \\frac{1}{2} \\left( \\gamma_0 + \\overline{Y}^2 + \\gamma_0 + \\overline{Y}^2 - 2 (\\gamma_{|t - s|} + \\overline{Y}^2) \\right) \\\\\n",
    "&= \\gamma_0 - \\gamma_{|t - s|} \n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  For the random walk, assuming without loss of generality $t \\leq s$,\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\Gamma_{t, s} &= \\frac{1}{2} \\text{E}[(Y_t - Y_s)^2] \\\\\n",
    "&= \\frac{1}{2} \\text{E}\\left[ \\left(\\sum_{j=t+1}^s e_j \\right)^2 \\right] \\\\\n",
    "&= \\frac{1}{2} \\text{E}\\left[ \\sum_{j=t+1}^s e_j^2 + \\sum_i \\sum_{j, j \\neq i} e_i e_j\\right] \\\\\n",
    "&= \\frac{1}{2} \\sum_{j=t+1}^s \\text{E} [e_j^2] + \\sum_i \\sum_{j, j \\neq i} \\text{E}[e_i] \\text{E}[e_j] \\\\\n",
    "&= \\frac{1}{2} (s - t) \\sigma_e^2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "while, for $t < s$, we can do $\\Gamma_{t, s} = \\Gamma_{s, t} = \\frac{1}{2} (t - s) \\sigma_e^2$\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$$ \\Gamma_{s, t} = \\frac{1}{2} |s - t| \\sigma_e^2 $$\n",
    "\n",
    "and so a random walk is intrinsically stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.27**.  For a fixed, positive integer $r$ and constant $\\phi$, consider the time series defined by $Y_t = e_t + \\phi e_{t - 1} + \\phi^2 e_{t - 2} + \\cdots + \\phi^r e_{t - r}$.\n",
    "\n",
    "**(a)** Show that this process is stationary for any value of $\\phi$.\n",
    "\n",
    "**(b)** Find the autocorrelation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**  We have:\n",
    "\n",
    "$$ \\text{E}[Y_t] \n",
    "= \\text{E}\\left[ \\sum_{j=0}^r \\phi^j e_{t - j} \\right] \n",
    "= \\sum_{j=0}^r \\phi^j \\text{E}[e_{t-j}] \n",
    "= \\sum_{j=0}^r \\phi^j \\cdot 0\n",
    "= 0\n",
    "$$\n",
    "\n",
    "which is constant (and does not depend on $t$).\n",
    "\n",
    "The variance is\n",
    "\n",
    "$$ \\text{Var}[Y_t] \n",
    "= \\text{Var}\\left[ \\sum_{j=0}^r \\phi^j e_{t - j} \\right]\n",
    "= \\sum_{j=0}^r \\phi^{2j} \\text{Var}[e_{t - j}] \n",
    "= \\sigma_e^2 \\sum_{j=0}^r \\phi^{2j}\n",
    "$$\n",
    "\n",
    "where this is a geometric sum for $\\phi^2 \\neq 1$ and a sum of $r + 1$ identical terms otherwise,\n",
    "\n",
    "$$ \\text{Var}[Y_t] = \\begin{cases}\n",
    "(r + 1) \\sigma_e^2 &\\text{for } \\phi^2 = 1 \\\\\n",
    "\\sigma_e^2 \\left( \\frac{1 - \\phi^{2(r + 1)}}{1 - \\phi^2} \\right) &\\text{otherwise}\n",
    "\\end{cases} \n",
    "$$\n",
    "\n",
    "Finally, the autocovariance for lag $k > 0$ is\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{Cov}[Y_t, Y_{t - k}] &= \\text{Cov}\\left[ \\sum_{i=0}^r \\phi^i e_{t - i}, \\sum_{j=0}^r \\phi^j e_{t - k - j} \\right] \\\\\n",
    "&= \\sum_{i=0}^r \\sum_{j=0}^r \\phi^{i + j}\\text{Cov}[e_{t - i}, e_{t - k - j} ]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The only non-zero terms in the sum occur when the indexes of both variables are the same, which is to say, $i = k + j$, with $i, j \\in [0, r]$.  Assuming $k \\leq r$, we can then write the somewhat simpler formula,\n",
    "\n",
    "$$\n",
    "\\text{Cov}[Y_t, Y_{t - k}] = \\sigma_e^2 \\sum_{i=0}^r \\phi^{2i - k} = \\frac{\\sigma_e^2}{\\phi^k} \\sum_{i = 0}^r \\phi^{2i}\n",
    "$$\n",
    "\n",
    "which is again a geometric sum for $\\phi^2 \\neq 1$ and a sum of $r + 1$ identical terms otherwise,\n",
    "\n",
    "$$ \\gamma_k = \\begin{cases}\n",
    "0 &\\text{for } k > r \\\\\n",
    "(r + 1) \\frac{\\sigma_e^2}{\\phi^k} &\\text{for } \\phi^2 = 1 \\\\\n",
    "\\frac{\\sigma_e^2}{\\phi^k} \\left( \\frac{1 - \\phi^{2(r + 1)}}{1 - \\phi^2} \\right) &\\text{otherwise}\n",
    "\\end{cases} \n",
    "$$\n",
    "\n",
    "As this is not a function of time either, the process $\\{ Y_t \\}$ is stationary for any value of $\\phi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  As we already computed the autocovariance, the autocorrelation is relatively straightforward,\n",
    "\n",
    "$$ \\rho_k = \\gamma_k / \\gamma_0 $$\n",
    "\n",
    "or\n",
    "\n",
    "$$\n",
    "\\rho_k = \\begin{cases}\n",
    "0 &\\text{for } k > r \\\\\n",
    "\\phi^{-k} &\\text{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.28 (Random cosine wave extended)**.  Suppose that\n",
    "\n",
    "$$ Y_t = R \\cos (2 \\pi (f t + \\Phi)) \\quad \\text{for } t = 0, \\pm 1, \\pm 2, \\dots $$\n",
    "\n",
    "where $0 < f < \\frac{1}{2}$ is a fixed frequency and $R$ and $\\Phi$ are uncorrelated random variables and with $\\Phi$ uniformly distributed on the interval (0, 1).\n",
    "\n",
    "**(a)** Show that $\\text{E}[Y_t] = 0$ for all $t$.\n",
    "\n",
    "**(b)** Show that the process is stationary with $\\gamma_k = \\frac{1}{2} \\text{E}[R^2] \\cos (2 \\pi f k)$.\n",
    "\n",
    "Hint: Use the calculations leading up to Equation (2.3.4), on page 19."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\text{E}[Y_t] &= \\text{E}[R \\cos (2 \\pi (f t + \\Phi))] \\\\\n",
    "& = \\text{E}[R] \\text{E}[\\cos (2 \\pi (f t + \\Phi))] \\\\\n",
    "&= \\text{E}[R] \\int_0^1 \\cos (2 \\pi (f t + \\phi)) d\\phi \\\\\n",
    "&= \\text{E}[R] \\frac{1}{2 \\pi} \\left[ \\sin (2 \\pi (f t + \\phi))\\right]_{\\phi = 0}^1 \\\\\n",
    "&= \\text{E}[R] \\frac{1}{2 \\pi} \\left[ \\sin (2 \\pi ft + 2 \\pi) - \\sin (2 \\pi f t) \\right] \\\\\n",
    "&= \\text{E}[R] \\frac{1}{2 \\pi} \\cdot 0 \\\\\n",
    "&= 0\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  The autocovariance is\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{Cov}[Y_t, Y_{t - k}] &= \\text{E}[Y_t Y_{t - k} - \\text{E}[Y_t] \\text{E}[Y_{t - k}] \\\\\n",
    "&= \\text{E}[R^2 \\cos (2 \\pi (f t + \\Phi)) \\cos (2 \\pi (f (t - k) + \\Phi))] - 0 \\cdot 0 \\\\\n",
    "&= \\text{E}[R^2] \\text{E}[ \\cos (2 \\pi (f t + \\Phi)) \\cos (2 \\pi (f (t - k) + \\Phi))]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "But $\\cos \\alpha \\cos \\beta = \\frac{1}{2} \\left( \\cos (\\alpha + \\beta) + \\cos(\\alpha - \\beta) \\right) $, so\n",
    "\n",
    "$$ \\cos (2 \\pi (f t + \\Phi)) \\cos (2 \\pi (f (t - k) + \\Phi)) = \\frac{1}{2} \\left( \\cos (2 \\pi fk) + \\cos(2 \\pi ( f (2t  - k) + 2 \\Phi)) \\right) $$\n",
    "\n",
    "and\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\text{E}[\\cos(2 \\pi ( f (2t  - k) + 2 \\Phi))] &= \\int_0^1 \\cos(2 \\pi f ((2t  - k) + 2 \\phi)) d\\phi \\\\\n",
    "&= \\frac{1}{4 \\pi} \\left[ \\sin (2 \\pi (f (2t - k) + 2\\phi))\\right]_{\\phi = 0}^1 \\\\\n",
    "&= \\frac{1}{4 \\pi} \\left[ \\sin (2 \\pi f (2t - k) + 4 \\pi) - \\sin (2 \\pi f (2t - k))) \\right] \\\\\n",
    "&= 0\n",
    "\\end{align}\n",
    "$$ \n",
    "\n",
    "Plugging it back into the covariance, we get\n",
    "\n",
    "$$ \\text{Cov}[Y_t, Y_{t - k}] = \\text{E}[R^2] \\frac{1}{2} \\left(\\text{E}[\\cos (2 \\pi fk)] + \\text{E}[\\cos(2 \\pi f (2t  - k) + 2 \\Phi)]\\right) = \\frac{1}{2} \\text{E}[R^2] \\text{E}[\\cos (2 \\pi fk ) ] $$\n",
    "\n",
    "Since this does not depend on $t$, and the mean is constant, the process is stationary, and the autocovariance is\n",
    "\n",
    "$$ \\gamma_k = \\frac{1}{2} \\text{E}[R^2] \\text{E}[\\cos (2 \\pi fk ) ]  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.29 (Random cosine wave extended further)**.  Suppose that\n",
    "\n",
    "$$ Y_t = \\sum_{j=1}^m R_j \\cos [ 2\\pi (f_j t + \\Phi_j) ] \\quad \\text{for } t = 0, \\pm 1, \\pm 2, \\dots $$\n",
    "\n",
    "where $0 < f_1 < f_2 < \\cdots < f_m < \\frac{1}{2}$ are $m$ fixed frequencies, and $R_1, \\Phi_1, R_2, \\Phi_2, \\dots, R_m, \\Phi_m$ are uncorrelated random variables with each $\\Phi_j$ uniformly distributed on the interval (0, 1).\n",
    "\n",
    "**(a)** Show that $\\text{E}[Y_t] = 0$ for all $t$.\n",
    "\n",
    "**(b)** Show that the process is stationary with $\\gamma_k = \\frac{1}{2} \\sum_{j = 1}^m \\text{E}(R_j^2) \\cos (2 \\pi f_j k)$\n",
    "\n",
    "Hint:  Do Exercise 2.28 first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**.  Let $A_{j, t} = R_j \\cos [ 2 \\pi f_j t + \\Phi_j ]$, and so $Y_t = \\sum_{j=1}^m A_{j, t}$.  We can now reuse the results from Exercise 2.28."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**\n",
    "\n",
    "$$ \\text{E}[Y_t] = \\text{E}\\left[ \\sum_{j=1}^m A_{j, t} \\right] = \\sum_{j = 1}^m \\text{E}[A_{j, t} = \\sum_{j = 1}^m 0 = 0 $$\n",
    "\n",
    "since $\\text{E}[A_{j, t}] = 0$ as per Exercise 2.28 (a)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\text{Cov}[Y_t, Y_{t - k}] &= \\text{Cov}\\left[ \\sum_{i=1}^m A_{i, t}, \\sum_{j=1}^m A_{j, t - k}\\right] \\\\\n",
    "&= \\sum_{i=1}^m \\sum_{j=1}^m \\text{Cov}[A_{i, t}, A_{j, t - k}] \\\\\n",
    "&= \\sum_{i=1}^m \\text{Cov}[A_{i, t}, A_{i, t - k}] \\\\\n",
    "&= \\sum_{i=1}^m \\frac{1}{2} \\text{E}(R_i^2) \\cos (2 \\pi f_i k)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "again, using independence between the variables $A_{i, t_1}, A_{j, t_2}$ for $i \\neq j$, and then using the result from Exercise 2.28 (b)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.30 (Mathematical statistics required)**.  Suppose that\n",
    "\n",
    "$$ Y_t = R \\cos [ 2\\pi (ft + \\Phi) ] \\quad \\text{for } t = 0, \\pm 1, \\pm 2, \\dots $$\n",
    "\n",
    "where $R$ and $\\Phi$ are independent random and $f$ is a fixed frequency.  The phase $\\Phi$ is assumed to be uniformly distributed on (0, 1), and the amplitude $R$ has a Rayleigh distribution with PDF $f(r) = re^{-r^2/2}$ for $r > 0$.  Show that for each time point $t$, $Y_t$ has a normal distribution.  (Hint: Let $Y = R \\cos[ 2\\pi (ft + \\Phi)]$ and $X = R \\sin [2 \\pi (ft + \\Phi)]$.  Now find the joint distribution of $X$ and $Y$.  It can also be shown that all of the finite dimensional distributions are multivariate normal and hence the process is strictly stationary.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**.\n",
    "\n",
    "Let $Z = (A, B)$ be a two-dimensional vector with components that are normally distributed, centered at 0, and independent.  Their density functions are\n",
    "\n",
    "$$ \n",
    "f_A(a) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-a^2 / 2(\\sigma^2)} \n",
    "\\quad \\text{and} \\quad\n",
    "f_B(b) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-b^2 / 2(\\sigma^2)} \n",
    "$$\n",
    "\n",
    "If $C$ is the length of $Z$, $C = \\sqrt{A^2 + B^2}$, the cumulative distribution function for $C$ is\n",
    "\n",
    "$$ F_C(c) = \\int \\int_{D_x} f_A(a) f_B(b) dA $$\n",
    "\n",
    "where the integral is over the disk $D_x = \\{ (a, b) : \\sqrt{a^2 + b^2} < x \\}$\n",
    "\n",
    "Translating the integral to polar coordinates, we get\n",
    "\n",
    "$$ F_C(c) = \\frac{1}{2 \\pi \\sigma^2} \\int_0^{2\\pi} \\int_0^c r e^{-r^2 / (2 \\sigma^2)} dr\\;d\\theta = \\frac{1}{\\sigma^2} \\int_0^c  r e^{-r^2 / (2 \\sigma^2)} dr $$\n",
    "\n",
    "This means that the probability density function for $C$ is the derivative of the function above,\n",
    "\n",
    "$$ f_C(c) = \\frac{d F_C(c)}{d c} = \\frac{c}{\\sigma^2} e^{-c^2 / (2 \\sigma^2)} $$\n",
    "\n",
    "We then have that $C$ follows Rayleigh distribution.  In particular, if we pick $\\sigma = 1$, we have that $C$ follows a Rayleigh distribution, and $A$ and $B$ -- the cartesian coordinates for the vector $Z$ -- follow normal distributions $N(0, 1)$.\n",
    "\n",
    "Now we can map this result back into our original problem:  By picking a phase $\\Phi$ uniformly and a vector magnitude $R$ according to the Rayleigh distribution, we end up with cartesian coordinates $A$ and $B$ that follow a normal distribution.  But we can express them explicitly as $A = R \\cos[ 2\\pi (ft + \\Phi) ]$ and $B = R \\sin [ 2 \\pi (ft + \\Phi) ]$, so $A$ and $B$ follow the same distribution as $X$ and $Y$ in the suggested hint.\n",
    "\n",
    "*Reference: https://en.wikipedia.org/wiki/Rayleigh_distribution*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
